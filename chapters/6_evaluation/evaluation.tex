\chapter{Evaluation}

% [Last and this chapter]

In the last chapter, the approach have been explained in detail.
The outcome of the approach are test runs which produce data.
This chapter is about two things:
Section X is consists the presentation, analysis and interpretation of the collected data, and in section X, the chosen approach which produces the data is discussed and evaluated.

% [In the next chapter]

In the next chapter X, limitations of this thesis are reflected and ideas and food for thought about possible continuations of this work are given.



%TODO show here 1-2 figures in big format
% add more percentages etc ? like increase by 30%...


% -------------------------------------------------------------

\section{Results of the Experiment}

This section starts with the comparison of the original website to the test website, and the comparison of test website to the variant which includes the tracking script at the top position.
After that, the variants within each independent variable are compared with each other.

% [General IV Interpretation]

The general observation is that the position nor the attribute of the tracking script impact performance, not negative or positive.
This can be explained with the async mechanics the tracking script of the include of the script
% only difference was visible when adding another script
% simple explained, as another resource has to be downloaded and the browser needs to wait for its execution

% Quality of collected data by RUM and synth. is difficult to asses as they are "only" performance data.
% When other data is tracked such as web analytics metrics it may be more obvious, e.g. to see effect on bounce rate.

% IV1
%many vendors suggest to insert in at the top of head, so that it is as quickly as possible available and data collection can start as soon as possible.


% IV2...



% [Diagrams]

The diagrams show the distribution of the data via kernel density estimations and the median values are displayed as vertical lines.
The idea of adding cumulative distribution functions into the plots was considered but withdrawn, as they clutter the diagram and do not add substantial information.
The diagrams are created with python and the visualization libraries matplotlib and seaborn.
The source code, as well as the plots and the raw data, are on GitHub.
All figures are in appendix X.

% fn https://matplotlib.org/
% footnote http://seaborn.pydata.org/
% footnote https://github.com/aram-yesildeniz/plots

% https://hpbn.co/primer-on-web-performance/#analyzing-real-user-measurement-data

% -------------------------------------------------------------


\subsection{Original Website vs Test Website without GA}
%TODO make metric names big or something

The aim of this comparison is to evaluate how good the test website approximates the original e-commerce website.

% [Page Weight]

The first metric to compare is page weight:
For the first view, when the website is loaded for the first time and no resource is in the browsers cache, the test website has a little bit more bytes and triggers more requests, especially for HTML and image resources, whereas the original website requests more JS and other resources.
For the repeat View, when the browser is loading the website for the second time and only requests resources that are not already in the browsers cache, much more bytes are downloaded and requests are made for the original website.
The reason for this lies in the webservers configuration, as described in section X.
The test website is hosted on GitHub Pages, which sets the cache control header to a caching supportive option.
The webserver of the original website on the other hand has a less cache affirmative policy, leading to more requests and downloads for repeat views.

% add figure ref

% [PLT]

The Page Load Times for the first view between the original and test website are very similar, whereas the data for the original website is less distributed, and the medians are almost identical.
The PLT is in the repeat view slightly faster for the test website and the difference in the medians is 595 ms.
This is expected, as the page weight for the test website is much smaller than the original website for the repeat view, and a "lighter" website is more likely to load faster.

% Figure \ref{figure:plt_original_test}:


% [FCP]

The data for the visual metric FCP resembles the data for PLT:
Where FCP is nearly similar for first view, the test website is slightly faster in repeat views than its original counterpart.

%Figure \ref{figure:fcp_original_test}:


% [Conclusion]

In conclusion, the test website is nearly identical in terms of page weight, PLT, and FCP, for the first loading of the website.
When viewing the website for a second time, the test website performance better, as more resources have been cached by the browser.

% add more here, like it is a good approximation ?
% add LCP / Speed Index ?

% -------------------------------------------------------------

\subsection{Test Website without GA vs Variant P1}

This section compares the test website which does not include any tracking script, and the variant P1 where the GA tracking script is included at top of the head section in the main html document. 
Hence, the only difference between the two websites is the tracking script, which is represented in the diagrams.

% [Page Weight]

The bytes in the page weight in the first view is identical, only that more bytes for JS are downloaded.
Those bytes are the tracking script.
For the requests, apart from more requests for JS, also more images and other resources are requested in the variant P1 and it can be assumed that those requests are trigged by the tracking script.

%figure X


% [PLT, LCP]

The values for the PLT, the metric which represents how long the navigation process takes place, are very similar for both versions, for the first and also repeat view.
Same goes for the visual metric LCP, which is nearly identical for both versions.

%figure X

% [Conclusion]

It can be concluded the adding the tracking script has nearly no effect on web performance.
Although the page weight gets slightly bigger with the tracking script included, this weight has no negative impact on the page load time nor user-centric performance metrics such as LCP.


% -------------------------------------------------------------

\subsection{IV Position: Variant P1 vs Variant P2 vs Variant P3}

In this section, the different values of the first independent variable are compared with each other and insights into the question if the position of the tracking script impacts the websites performance are given.

% [DOM Content Loaded, PLT]

The navigation timing metrics DOM Content Loaded and PLT are nearly identical for all variants across first and repeat views.
The vertical "GA" lines in the figures report the values measured in RUM by Google Analytics.
As GA is not differentiating between first and repeat views (at least within the site speed reports), the values are for both cases the same and they depict here the average over all views.
For all results within this setup, no different data is available for RUM measurements from GA depending on whether the website was loaded the first time or from cache.
GA reports slightly faster navigation timing metrics for position 3, but the differences are marginal and conclusions cannot be drawn.

% [Speed Index, LCP]

The same picture emerges for visual and user-centric metrics, where for all variants, the measured values are nearly the same.
Outstanding is the fact that the overall performance, is much better in the repeat view, namely 3.5 seconds for Speed Index and around 1 second for LCP.

% [Conclusion]

With the given data, one may conclude that the position of the tracking script has no impact on performance.


%TTFB: - Repeat View is slower (on median) ??

% -------------------------------------------------------------

\subsection{IV Attribute: Variant A1 vs Variant A2 vs Variant A3}

The assumption is that the attribute has no effect on the performance, as discussed in section X.
The data reveals that this assumption holds true:

For navigation timings metrics such as PLT and Fully Loaded as well as visual metrics such as Speed Index and LCP, the measured values over all variants are alike and the attribute of the GA tracking script has no impact on the performance.
Worth mentioning is the observation that again, repeat views are always faster for all variants.

%TODO ref figures

%CLS: medians are all the same ??
%FV: A1 seems to have less distribution, where as A2 and A3 have some distributed data


% -------------------------------------------------------------

\subsection{IV Other Script: Variant OS1 vs Variant OS2}

%TODO add more metrics here where i also have GA values ?? to show that this is also visible in the RUM

Observing the data, it seems that the only independent variable that affects performance is the one which adds an additional script to the website.
This is expected as with an additional script, the page weight of the website increases and additional bytes have to be downloaded and requests have to be made.
The other two independent variables only alter how the resource is being loaded, but they do not change the page weight.

% [PLT, Fully Loaded]

In terms of navigation timings, the first view PLT is visibly lower for the variant without an additional script, and the median is increased by about 65 % when adding another script to the website.
GA also reports a difference by nearly 1 second.
In repeat view, the differences are much smaller, but the tendency is still observable.
A similar picture is reported on the Fully Loaded metric, where, especially in first view, the variant without an additional script performs better.

% [Speed Index, TBT]

Also in terms of visual performance measured by the Speed Index, the variant without the additional script performance better, especially for the first view.
In the repeat view, the distribution of the Speed Index has bimodal tendencies.
In fact, for all repeat views, Speed Index has a slightly bimodal distribution.
Further investigations are needed to find reasons for this behaviour.
They may lie in how WebPageTest is measuring the website.
The interactivity, as reported by the TBT metric, also performs better in the variant where no additional script is included.



% ----------------------------------------------------------------------------------------------------

\section{Evaluation of the Approach}

% Quality of the collected data.
% this is part of research question: do the different variants change the collected data ?


% For each attempt, describe: Threats to validity, generalizability


% -------------------------------------------------------------

% [Generalizability]

% meine Daten zeige nur für Chrome, MacBook, diese Geschwindigkeit etc.
% Und auch nur für diese Test-Website
% Die Schwierigkeit der Generalisierbarkeit ist eines der grössten Probleme bei dieser Fragestellung




% -------------------------------------------------------------

% [Internal and External Validity]

% The quality and quantity of the data needs to be discussed
% Quality: There are chances that some data are malformed, e.g. because internet connection was bad, etc.
% Quantity: Is the amount of data sufficient to make the evaluation generalisable



% 2016 Kohavi: Analysis of Experiments
















% ----------- To big to fail... to delete :/ -----------


% \subsection{Bulk Test Overview: Description of test result page}

% \begin{itemize}
% \item Each test has Test ID: YYMMDD\textunderscore random\textunderscore random
% \item Test results after bulk test available under \url{http://localhost:4000/result/{testID}/}
% \item For each test run, following data is available:
% 	\begin{itemize}
% 	\item Link to test results: Test result page as same as for single test run
% 	\item Median load time (First view)
% 	\item Median load time (Repeat view)
% 	\item Median Speed Index (First View)
% 	\item Raw page data (file: [TestID\textunderscore summary.csv]
% 	\item Raw object data (file: [TestID\textunderscore details.csv])
% 	\item Http archive (.har) (file: json)
% 	\end{itemize}
	
% \item Average First View Load Time
% \item Average Repeat View Load Time
% \item Combined Raw: Page Data  (file: [TestID\textunderscore summary.csv])
% \item Combined Raw: Object Data (file: [TestID\textunderscore details.csv]). For 100 test runs, this file is appr. 20 MB, 24432 rows, 76 columns. 
% \item Aggregate Statistics (file: [TestID\textunderscore aggregate.csv])
% \end{itemize}




% \subsection{Compare Section}

% WPT has a feature to compare multiple tests.
% Accessible under compare URL: \url{http://localhost:4000/video/compare.php?tests={TestID},{TestID},...}

% The compare page contains:

% \begin{itemize}
% \item Film strip
% \item Waterfall diagram
% \item Visual Progress diagram
% \item Timings diagram:
% 	\begin{itemize}
% 	\item Visually Complete (First View Visually Complete Median)
% 	\item Last Visual Change
% 	\item Load Time (onload)
% 	\item ...
% 	\end{itemize}
% \item Cumulative Layout Shift diagram
% \item Requests diagram
% \item Bytes diagram
% \item Visually complete
% \item Last Visual Change
% \item Load Time (onload)
% \item Load Time (Fully Loaded)
% \item DOM Content Loaded
% \item Speed Index
% \item Time to First Byte
% \item Time to Title
% \item Time to Start Render
% \item CPU Busy Time
% \item 85\% Visually Complete
% \item 90\% Visually Complete
% \item 95\% Visually Complete
% \item 99\% Visually Complete
% \item First Contentful Paint
% \item First Meaningful Paint
% \item Largest Contenful Paint
% \item Cumulative Layout Shift
% \item html Requests
% \item html Bytes
% \item js Requests
% \item js Bytes
% \item css Requests
% \item css Bytes
% \item image Requests
% \item image Bytes
% \item flash Requests
% \item flash Bytes
% \item font Requests
% \item font Bytes
% \item video Requests
% \item video Bytes
% \item other Requests
% \item other Bytes
% \end{itemize}




