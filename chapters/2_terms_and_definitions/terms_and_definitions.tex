% Terms and Definitions
%TODO maybe move stuff from chapter 1 in this chapter
\chapter{Metrics and Measurement Methods}

[tbd]

\begin{itemize}
	\item Last chapter...
	\item This chapter: In this chapter, I will cover measurement methods and discuss common performance metrics.
	\begin{itemize}
		\item This chapter should cover all relevant terms and definitions within web performance measurement
		\item How terms can be structured / taxonomy
		\item Ambiguity of definitions
	\end{itemize}
	\item In the next chapter...
\end{itemize}


- Technical Background:
	- Network
	- Front End: Navigation and CRP
	
- Metrics

- Measuring Methods



% https://developer.mozilla.org/en-US/docs/Learn/Performance/Measuring_performance
- Compare to competitors
- Compare different versions of your app
- Metrics should be relevant to your users, site, and business goals
- should be collected and measured in a consistent manner
- analyzed in a format that can be consumed and understood by non-technical stakeholders



% 2004 Phippen



% TODO
% IF possible use more cool tables


% ---------------------------------------------------------------------------------------------------
% ---------------------------------------------------------------------------------------------------


%TODO find better title
\section{How Websites are being loaded}

% [Introduction]
- Brief technical introduction
- It is important to understand how things work, because Metrics and also how to measure them are derived from those processes

In order to understand web performance metrics and the methods to measure them, it is crucial to have a basic understanding of the technical aspect of loading a website into a browser.
This process includes establishing a connection between a client and a server, which will be discussed in section X, and the task of the browser to transform the received data from the server into a readable ready-to-use website, which will be discussed in section X.
Always with performance in mind.

% [3 Entities: FE, BE, Network]
It is possible to divide the website loading process into three entities which play a role.
In his code talk 2016, Witt identifies three main areas or bottlenecks where bad performance is being produced: In the Frontend, the Backend, and on the network layer.  % cite 2016 Witt or just say this ??
The Front End is everyhting the user sees on the screen, client, UI, browser, sends requests to a back end, etc.
The Back End is the logic, servier, also data base, handles requests and sends responses to a front end
Network is what connects clients and servers, FE and BE, infrastructure element composed of routers, cables, wireless connections etc.


- BE is not discussed (server time, data base, etc.)
- Section X is about Network
- Section X is about Front end: how browser works, crp, 
- How to optimise websites is not part of this thesis

In this section, i will also say which metrics are describing the underlying process.
So this section links directly to the metrics section.
But all metrics are collected in the metrics section.



% --------------------------------------------------------------------------------------------
% --------------------------------------------------------------------------------------------


%TODO choose title. Maybe "Establishment of Connection?"
\subsection{The Network Realm}

Starting from hardware, ISP, routers, switches etc and the cables connecting them is part of the network.
But also communication protocols such as the Internet protocol suite.

Regarding performance, latency and bandwidth come into mind, and we will see that latency has a bigger impact on performance than bandwidth in section X.

After discussing this issue, i will continue by describing the process or navigation steps which happen once the user enters a URL into the browser, up until he sees pixels on his screen and can use the website.


% --------------------------------------------------------------------------------------------


\subsubsection{Latency and Bandwidth}

There are two important attributes when discussing network performance: Latency and Bandwidth.
The important thing to say here is that Latency is bottleneck for performance, and not bandwidth.


% [Bandwidth]

Bandwidth is the "maximum throughput of a logical or physical communication path". %cite 2013 Grigorik
In other words, bandwidth describes the amount of data which can be sent in parallel from one node in a network to another. 

Physical communication paths are most likely cables such as metal wires or fiber-optic cables, where fiber-optic cables have less signal loss, and lower lifetime maintenance costs.
With methods such as wavelength-division multiplexing (WDM), it is possible to transmit up to 70 Tbit/s over a fiber-optic connection.  %cite 2013 Grigorik

This high technology stuff is only used in the backbone infrastructure, e.g. for connecting Europe with America.
For the end user, bandwidth is much lower, and the average was in late 2015 just 5.1 Mbps %cite 2013 Grigorik

A high bandwidth is useful for bulk or large data transfer such as streaming of video or audio.
But for loading a website,or any browser activity that depends on many requests that fetch data from many different locations around the globe, the performance bottleneck is latency. % cite 2013 Grigorik


% [Latency]

Latency is "the time from the source sending a packet to the destination receiving it".  % cite 2013 Grigorik

Latency is measured in seconds and can be the time spent for one-way, or more common, how long it takes for the transmitted data package for the round-trip time (RTT), from source to destination and back.
In other words, latency "describes the amount of delay on a network or Internet connection". % cite https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency

For the very first request when establishing a connection, latency is longer due to protocols such as DNS lookup, TCP and TLS handshakes.
Those will be discussed in section X. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency


% [Experiment]

To get an idea about how the two aspects, bandwidth and latency, impact web performance,  Mike Belshe launched a study. % cite https://docs.google.com/a/chromium.org/viewer?a=v&pid=sites&srcid=Y2hyb21pdW0ub3JnfGRldnxneDoxMzcyOWI1N2I4YzI3NzE2
Once setup has a fixed latency and bandwidth is variable, and vice versa.
He and compared the performance of the two experiments using the Page Load Time metric. (cf. X for this metric)


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.8\textwidth]{latency.png}
\caption{Latency vs Bandwidth}
\label{img:latency}
\end{center}
\end{figure}


We can see that the impact of bandwidth is trivial: if the available bandwidth is doubled, e.g. from 5 to 10 Mbps, there is no change in performance load time.
For Latency on the other hand, the picture is different: If the latency can be decreased by half, e.g. from 120 ms to 60 ms, the page load time also sinkt um die hälfte.

Or as Belshe states it, "[reducing] cross-atlantic RTTs from 150ms to 100ms [...] would have a larger effect on the speed of the internet than increasing a user's bandwidth from 3.9Mbps to 10Mbps or even 1Gbps." % cite https://docs.google.com/a/chromium.org/viewer?a=v&pid=sites&srcid=Y2hyb21pdW0ub3JnfGRldnxneDoxMzcyOWI1N2I4YzI3NzE2

This obersvations can be explained with the many short, small connections and requests are made when browsing websites and the contrary underlying structure of the communication protocols, which are "optimized for long-lived connections and bulk data transfers. " %cite 2013 Grigorik ch 10

But just simply decreasing the latency is not straightforward: The speed of data transfer is already at a 2/3 of light, but the physical constraint is the limiting factor, e.g. there is a minimum distance between London and New York which can not be further "optimized". % cite 2013 Grigorik Ch 1


% [Mobile]

Another aspect of latency is that for wireless connections and therefore mobile devices, latency is even higher, "making networking optimization a critical priority for the mobile web." % cite 2013 Grigork Ch 1

This is due to the infrastructure of mobile nets, latency is high for mobile users. cf.  Why are mobile latencies so high? in Grigorik % cite https://www.igvita.com/slides/2013/fluent-perfcourse.pdf


% [Transition]

As latency is a important factor, what happens on the front end is still important.
And again for this thesis metrics measuring performance in the front end are the focus.

Before i will discuss what happens in the browser once the website data arrived, i will briefly describe the preceding steps of establishing a connection between the browser (client) and the server.



%TODO add this ?

% Use other techniques such as CDNs, caching, pre-fetching, etc % 2013 Grigorik
% CDN: Help against this issue. Put stuff close to client % 2013 Grigorik

% Some direct implications for performance measurement ?
% Understanding Latency https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency
% Network throttling: Emulate download speed, upload speed, and minimum latency



% --------------------------------------------------------------------------------------------



\subsubsection{Navigation Process}

I will explain briefly the general navigation process: It begins when the user is submitting a URL in the browser and ends when he received website data.

The main steps can be divided into networking, that is, establishing a connection with DNS etc., backend processing, e.g. data base queries etc., and the rendering in the front end, as seen in image X.
The last part of this process is when browser receives finally the HTML / Document. 
How the browser transfers the HTML into an interactive website is part of the next section.


"To start, it is important to recognize that every HTTP request is composed of a number of separate stages (Figure 10-3): DNS resolution, TCP connection handshake, TLS negotiation (if required), dispatch of the HTTP request, followed by content download." % cite Grigorik 2013


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.8\textwidth]{timing_overview.png}
\caption{Timing Overview}
\label{img:timing_overview}
\end{center}
\end{figure}
%TODO change image: remove network and BE boxes? remove white space around image


%TODO add this ?
% Understanding Latency https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency
%Network Timings:
%- Blocked: When a request is in queue
%- Blocking happens when there are too many simultaneous connections to single server over HTTP



% ----------------------------------


\paragraph{DNS Lookup}

When the requested resource can not be loaded from the browsers cache, the first step to establish a connection is a DNS Lookup (or DNS Resolution).

This step is about translate URL to IP address.
Must be done for each unknown URL, e.g. when linked images within a website are from different server, for each unique URL DNS Lookup has to be done.
The mapping of URL to IP can be cached by browser, which makes repeated views faster. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work

Avg. time is 20 and 120 ms % https://www.keycdn.com/support/reduce-dns-lookups
Can be considered a performance metric, see section X.


% ----------------------------------


\paragraph{TCP Handshake}

Once a connection between a client and a server is established, the TCP 3-way-Handshake comes into play.

The goal of TCP is to establish a reliable connection within an unreliable network.
TCP  "guaranteed that all bytes sent will be identical with bytes received and that they will arrive in the same order to the client. " %cite 2013 Grigorik
Regarding performance, the handshake adds two more round trips, which is bad for performance as we have seen because of latency.

Many algorithms and techniques to get optimal data transfer and also avoid congestion are existing, such as Slow-Start.
Slow-Start is an algorithm that determines the maximum bandwidth that can be used by gradually increasing the amount of data sent.
Slow start prevents that the full capacity of the network is being used from the beginning, which in performance terms adds again more round trips and latency. %cite 2013 Grigorik


%TODO once i know which metric add it here
A performance metric reflecting the time spent for establishing a TCP connection is X, see section X.
For a detailed discussion cf "Building Blocks of TCP" in 2013 Grigorik % cite https://hpbn.co/building-blocks-of-tcp/


% ----------------------------------


\paragraph{TLS Negotiation}

TLS is another protocol which has the goal to establish a secure connection in terms of data encryption.
Data transmitted over the network has to be encrypted so that aussenstehende can not read or manipulate the data.
For encryption,  a cipher to be used needs to be established, which will be shared between client and server during the TLS Negotiation. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work

TLS again adds more round trips which is bad for performance.

%TODO once i know which metric add it here
A performance metric reflecting the time spent for a TLS negotiating is blabla in section X.

for a detailed discussion see Transport Layer Security (TLS) in 2013 Grigorik % https://hpbn.co/transport-layer-security-tls/


% ----------------------------------


\paragraph{HTTP Request and Response}

Now that a secure connection is established, the client fetches the first resources via HTTP GET request.
Most often, the server will respond by sending back the index.html file, which then can be used by the browser to build the website. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work

The time when this first response containing the first byte for building the web site is reflected in the metric TTFB which is discussed in section X.


% [Connection vs Request]

Usually, many more resources are requested by the browser to complete the build of the web site.
As of today, the median value is about 70 requests per web site. % footnote https://httparchive.org/reports/state-of-the-web#reqTotal

A request is not the same as a connection.
Once the connection is established via the above described procedures such as DNS lookup, TCP and TLS handshakes, multiple requests can be transmitted over the same connection.
Usually, the number of requests is much higher than the number of connections to load a website, as the browser persist connections, keep them open for multiple requests.
Median connections for a web site today is about 13. % footnoe https://httparchive.org/reports/state-of-the-web#tcp
Modern browsers like Chrome enable up to six open connections in parallel. % cite 2014 Hogan



% [Transition to CRP]

At this point, the browser has received the first data about the web site and he can start with rendering the page.
How this exactly happens, is explained in the next section.




% --------------------------------------------------------------------------------------------
% --------------------------------------------------------------------------------------------



\subsection{Front End: Critical Rendering Path}

This section explains what happens after the first bytes of the web sites arrived in the browser.
The following processes are typically subsumed under the term \textit{Critical Rendering Path} (CRP).
The CRP is the last part of the navigation process as seen in image X.


% [Critical]

The CRP is the minimum steps that the browser has to take from the moment it receives the first byte of HTML to the moment that it renders pixels on the screen for the first time.

The rendering is critical as it is the very first render, the first visible content the user will see on the screen.
The resources that are needed for the first render of the page delay the first render of the page are considered to be critical.
Without the critical resources, the browser can not display content on the screen.
An example of a critical resource is the first HTML file the browser receives, as without it, nothing is visible on the screen.
Non-critical resources on the other hand will not stop the browser from displaying the first content on the screen. % cite https://blog.logrocket.com/how-css-works-parsing-painting-css-in-the-critical-rendering-path-b3ee290762d3/


% https://gtmetrix.com/blog/how-to-eliminate-render-blocking-resources/
%- Non-critical resources are those that provide contributions to secondary/tertiary functionality or styling for the content on your page, e.g., a calendar widget on the sidebar below-the-fold.


% https://blog.logrocket.com/how-css-works-parsing-painting-css-in-the-critical-rendering-path-b3ee290762d3/
%- Any CSS that is not necessary for the first load can be considered “non-critical”



% [CRP Steps]

There are a sequence of steps the browser goes through to render the page.
The basic idea is to convert HTML, CSS and JS to actual pixels on the screen.

Image X visualizes the flow of the CRP:
Once the HTML is received, the browser starts with parsing the HTML and translate it into the DOM.
The content of the CSS files will be parsed to the CSSOM.
JavaScript needs to be fetched and executed.
Once DOM and CSSOM are available, the Render Tree is being created.
When the Render Tree is available, Layout is happening.
Finally, pixels can be printed on the screen.
% cite https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path

In the following, the individual steps will be discussed in more detail.


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{crp.png}
\caption{Critical Rendering Path}
\label{img:crp}
\end{center}
\end{figure}




% [Single Thread]
%TODO do i need this ?
% How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
%- This is somewhat the bottleneck or the technical state of the browser
%- Browser is single threaded
%- Still: Enable smooth interaction: scrolling, responsive to touch, etc.
%- Render time is key
%- Goal: Main thread can complete all the work and still is available to handle user interaction
%-> Improvement: Understand single thread concept of browser and minimize main threads responsibilities
%-> Should lead to: rendering is fast and smooth and responses to interactions are immediate



\paragraph{DOM Construction from HTML}

% [Introduction, Standard]

Once the browser received the first bytes of the HTML file, it starts to parse it into the \textit{Document Object Model} (DOM).
The DOM construction is the first step the browser performs when receiving data.
The DOM is a tree structure and internal representation of the HTML for the browser. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
The general parsing process consists of translating from bytes to characters, to tokens, to nodes and finally to the object model.% cite https://developers.google.com/web/fundamentals/performance/critical-rendering-path/constructing-the-object-model
The specification of the DOM is maintained by the WHATWG living standard. % footnote https://dom.spec.whatwg.org/

% The Parsing of the HTML into the DOM is defined in the HTML standard % footnote https://html.spec.whatwg.org/multipage/parsing.html#parsing


% [Render Blocking]

The DOM tree contains information about the content of the document, but not its style.
The styling is defined in the CSS.
Once HTML and CSS are transmitted and processed by the browser, the \textit{Render Tree} can be created, which reflects the actual information and its styling the browser can display.
Within this context, it is possible to categorise resources into render blocking and non-render blocking.
A render blocking resource is a resource that prevents the browser from rendering content to the screen.
HTML and CSS are render blocking resources, as the parsing process of those files blocks the browser of displaying the page to the screen.% cite https://developers.google.com/web/fundamentals/performance/critical-rendering-path/render-blocking-css

% CSS parsing and render tree construction will be discussed below.


% [Incrementally]

As soon as the first data packages of HTML arrive at the browser, the parsing process starts. %cite How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
The DOM is created incrementally,  this means that the browser can begin to process the HTML before all of its content is transmitted over the network.


% [Resources]

Usually, within the HTML, external resources are linked which are necessary for the website to be complete, such as CSS or JavaScript.
While parsing the HTML incrementally, eventually a reference to such an external resource will be encountered.
How the external resources CSS and JavaScript are being handled by the browser is discussed below.


%TODO add this?

% How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
%- DOM is also exposed, and can be manipulated through various APIs in JavaScript 
%Optimisation: Preload scanner:
%- This process occupies main thread while browser is building DOM Tree
%- parse through the content available and request high priority resources like CSS, JavaScript, and web fonts.
%- will retrieve resources in the background so that by the time the main HTML parser reaches requested assets, they may possibly already be in flight, or have been downloaded




% ------------------------------------------------------------


\paragraph{CSSOM Construction from CSS}


% [Introduction]

The CSS resource contains all information about the styling of the page.
As with the HTML,CSS is converted from bytes to characters, to tokens, to nodes, and finally to the \textit{CSS Object Model} (CSSOM). % cite https://developers.google.com/web/fundamentals/performance/critical-rendering-path/constructing-the-object-model
CSSOM construction is usually very fast . % cite % How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
CSSOM is standardized here % footnote https://drafts.csswg.org/cssom/

% The DOM and and CSSOM are separated structures and not yet connected.
% The Render Tree reflects the combination of the two models and will be discussed below.
% Creation of CSSOM happens after DOM is completed. % https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path



% [Cascading, Not incrementally]

As opposed to the HTML parsing process, CSS can not be translated to the CSSOM incrementally.
Cause it the cascading nature of style sheets, which has the potential that the styling rules defined at the top of the file may be overridden by rules defined at the very end of the CSS file.
A partial CSSOM is therefore not possible.
Hence the browser needs the entire CSS file before he can create the CSSOM.
%cite  https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path


% [Not Parser Blocking]

As soon as the parser encounters a reference to an external style sheet such as

%TODO add caption={} ?
\begin{lstlisting}[language=html, numbers=none]
<link rel="stylesheet" href="styles.css">
\end{lstlisting}

it requests the resource and continues with parsing the HTML.
CSS is not a parser blocking resource.
When the CSS arrived at the browser, the CSSOM construction starts.
%cite  https://blog.logrocket.com/how-browser-rendering-works-behind-the-scenes-6782b0e8fb10/


%TODO add FOUC ?

% https://blog.logrocket.com/how-css-works-parsing-painting-css-in-the-critical-rendering-path-b3ee290762d3/
% - If it just went ahead and rendered to pixels without waiting for the CSSOM we’d see a flash of unstyled content (ugly!) for a moment while the CSSOM was parsing.



% [Render Blocking]

While CSSOM creation is not parser blocking, it is render blocking.
The browser blocks the page rendering until it received and parsed all of the CSS.
Rendering content to the screen is only possible when CSSOM and therefore CSS is available. % cite https://developers.google.com/web/fundamentals/performance/critical-rendering-path/render-blocking-css


% [When finished]

Once the DOM and CSSOM are created, they can be merged together into the render tree, which will be layout and painted to the screen.
Before I describe this process, I will discuss how JavaScript is being handled.


% [Optimization]

%TODO add this: optimization
%- You want to get CSS down to the user as quick as possible:
	%- Inlining styles
	%- Load not needed styles later
%- Optimization: use media queries
%- Better to add styles in single file
%- FOUC: Flash of Unstyled Content: If unstyled content is visible on the screen


% https://developers.google.com/web/fundamentals/performance/critical-rendering-path/render-blocking-css
%Media types and media queries allow us to mark some CSS resources as non-render blocking.


%TODO add this? Lighthouse what is render blocking https://web.dev/render-blocking-resources/
%Lighthouse flags resources as render blocking when:
%A <link rel="stylesheet"> tag that:
%Does not have a disabled attribute. When this attribute is present, the browser does not download the stylesheet.
%Does not have a media attribute that matches the user's device.





% ------------------------------------------------------------



\paragraph{JavaScript in the CRP}


% [Introduction, Parser and Render Blocking]

JavaScript (JS) resources add functionality and interactivity to a web site.
When the browser encounters a script tag such as

%TODO add caption={} ?
\begin{lstlisting}[language=html, numbers=none]
<script src="myScript.js"></script>
\end{lstlisting}

it will stop its current task of parsing, fetch immediately the resource and execute its content, and only then proceed with the creation of the DOM. % cite % Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
See image X.

JS can manipulate and query the DOM tree and directly change the HTML file.
As the HTML file is the input stream for the parser,  the parser stops until the JS is downloaded and executed. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path
Hence JS is parser blocking.
JS fetching and execution stops the parsing of the HTML and the construction of the DOM.
Only after the script finished execution, HTML parsing will continue.

Implicitly, because JS execution blocks DOM creation, and HTML processing itself is render blocking, JS is also render blocking. % cite Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik


The behaviour is the same for an external references JS file and a script directly added within in the HTML.


%TODO add this?

% This means that excessive scripts can be a significant bottleneck % How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work

% https://blog.logrocket.com/how-browser-rendering-works-behind-the-scenes-6782b0e8fb10/
% - If the network is slow, and it takes thousands of milliseconds to fetch app.js, the DOM construction will be halted for the thousands of milliseconds as well

% https://blog.logrocket.com/5-tricks-to-eliminate-render-blocking-resources/
%- You can remove them from the critical rendering path by placing the <script> tags right before the closing </body> tag instead of the <head> section.
%-In this case, they only begin to download after the entire HTML has been downloaded.

% https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
%- Though the browser's preload scanner hastens this process.


% [Blocked by CSS]

As JS can also manipulate the styling of the page, its execution is blocked until the CSSOM is available.
This means that the execution of the JS is on hold until the CSSOM is ready.
To summarize, while JS blocks the parsing of the HTML to the DOM, JS execution itself is blocked by the creation of the CSSOM.
CSSOM blocks JS, and JS blocks DOM construction. %cite 2013 Grigorik ch 10

Several attributes on the script tag can change the behaviour of the browser.
\textit{Async} and \textit{defer} are options to counter the blocking nature of the script tag.
They will be discussed now.\\



% [async attribute]

With the async (asynchronous) attribute, the browser downloads the JS in the background while continuing with the parsing of the HTML.
The parsing is not blocked and the browser can continue with his task. 
As soon as the JS is downloaded and available,  it is parser blocking: the browser stops the parsing and executes the JS.%cite https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script

The order of all the async scripts within the document is not maintained any more.
Whenever a script is downloaded and available, it will be executed.
It does not matter if an async script was included at the top or bottom of the HTML document. % cite https://blog.logrocket.com/5-tricks-to-eliminate-render-blocking-resources/



% https://blog.logrocket.com/5-tricks-to-eliminate-render-blocking-resources/
%- The async attribute is recommended for independent third-party scripts, such as ads, trackers, and analytics scripts. For example, Google Analytics recommends adding the async attribute to support asynchronous loading in modern browsers.


% [defer attribute]

Like with async, scripts with the defer attribute enable the browser to download the script in parallel while continuing with the parsing of the HTML.
Contrary to async, defer scripts will only be executed after the parsing of the page is complete and the DOM tree is fully constructed,  and the order of the scripts will be maintained.  %cite  https://blog.logrocket.com/5-tricks-to-eliminate-render-blocking-resources/


%TODO add this nice graphic and cut out last rows https://html.spec.whatwg.org/multipage/scripting.html#attr-script-async
\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.8\textwidth]{scripts.png}
\caption{Scripts}
\label{img:latency}
\end{center}
\end{figure}


The async and defer attributes not applicable on inline scripts.
From the standard, "scripts may specify defer or async, but must not specify either unless the src attribute is present. " % \footnote https://html.spec.whatwg.org/multipage/scripting.html#attr-script-async
As inline scripts do not contain a src attribute, as the source of the script is within the script tags, the async and defer attributes are not applicable.


% https://blog.logrocket.com/5-tricks-to-eliminate-render-blocking-resources/
%- The defer attribute is recommended for scripts that need the DOM, but you want to begin to download them before the document loads, without making them a render blocking resource.
%- You should also use defer rather than async if the document order is important — for instance, when consecutive scripts depend on each other.



% [Transition]





% [More stuff]

%TODO add this? preload scanner
% https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
%- Though the browser's preload scanner hastens this process.

% Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
% And slides https://www.igvita.com/slides/2013/fluent-perfcourse.pdf
%- Asynchronous pattern:
	%- Not the same as async/defer tag
	%- Will fetch JS asynchronously
	%- Uses IIFE which creates a new script tag in the HTML with attribute async
	%see chapter X how Google Analytics is doing this.


%TODO add this? https://web.dev/render-blocking-resources/
%Lighthouse flags resources as render blocking when:
%A <script> tag that:
%Is in the <head> of the document.
%Does not have a defer attribute.
%Does not have an async attribute.


%TODO add this? resource hints
% https://blog.logrocket.com/using-resource-hints-to-optimize-performance/





% ------------------------------------------------------------




\paragraph{Building the Render Tree}


As already described above, HTML and CSS are both render blocking, as they prohibit the rendering of the page.
Rendering can happen once the \textit{Render Tree} is available.
The render tree is the combination of the DOM and CSSOM and captures all visible content with its styles which will be displayed on the screen.
If an element has a CSS property such as \verb|display: none;| it will not occur in the render tree. % cite https://developers.google.com/web/fundamentals/performance/critical-rendering-path/render-tree-construction

The computed render tree is then used to layout the content to the page, which is described in the next paragraph.


%TODO add this ? Check again when chapter about WPT metrics is done
% 2014 Hogan https://designingforperformance.com/
%Chapter 2
%- Start Render Metric in WPT




% ------------------------------------------------------------


\paragraph{Layout}


% [Introduction]

In the layout process, the position and size of the nodes from the render tree are calculated.
New layout calculations or reflows are triggered as soon as the screen area changes, e.g. by device rotation or window resizing, or on modifications of the DOM and render tree. % cite How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work

Once the layout is resolved, the browser continues with painting pixels on the screen.


%TODO add viewport? The projection area is dependent and defined by the viewport

% https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path
%- The viewport meta tag defines the width of the layout viewport, impacting the layout.
%- Without it, the browser uses the default viewport width, which on by-default full screen browsers is generally 960px. On by-default full screen browsers, like your phone's browser, by setting <meta name="viewport" content="width=device-width"



% ------------------------------------------------------------


\paragraph{Paint}

Finally, the browser can paint the content on the screen.
If some content changes, browsers are optimized to only repaint areas on the screen affected.  %cite  https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path



%TODO add metrics ?
% How Browsers Work https://developer.mozilla.org/en-US/docs/Web/Performance/How_browsers_work
%-> First Meaningful Paint
%- Time to Interactive


%TODO add above the fold?
% https://gtmetrix.com/blog/how-to-eliminate-render-blocking-resources/
%- Above the fold: “Above-the-Fold” refers to the area that the visitor normally sees on a website before scrolling down to see the rest of the content. 



%TODO add performance question?
% https://blog.logrocket.com/how-css-works-parsing-painting-css-in-the-critical-rendering-path-b3ee290762d3/
%- Paint: It’s important to remember that some CSS properties can have a larger impact on the page weight than others (for example, a radial-gradient is much more complex to paint than a simple color).




% ------------------------------------------------------------


%TODO add this ?

% [Continuos Loop, 60 frames per second]

% [Compositing ?]


% ------------------------------------------------------------


\subsection{Conclusion Technical Background}

[tbd]


% We could see that performance Metrics are directly derived from this process
% Metrics will be discussed next
% After that, i will talk about how to actually measure the metrics



%TODO add ?
% [Optimizations]

% Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
% And slides https://www.igvita.com/slides/2013/fluent-perfcourse.pdf
%- Optimize the critical rendering path!
%-> styles at the top, scripts at the bottom best practice
% - Different browsers implement different logic for when, and in which order, the individual resource requests are dispatched. As a result, the performance of the application will vary from browser to browser. 2013 Grigorik ch 10



% 2014 Hogan https://designingforperformance.com/
%- Optimizations of CRP:
%- media types and queries on css resources, which makes them non-blocking
%- Load JS efficient
%- Priotize requests for above the fold
%- etc. % Do i need to explain this ??
%Chapter 4:
%CSS and JavaScript Loading:
%- Rules:
%- Load CSS in head
%- CSS blocks rendering
%- Load JS at bottom of the page
%- Load Async
%- JS blocks DOM construction, because browser knows that content from script tag may change Render Tree
%- async tag will execute script once its ready, but order is not berücksichtigt
%- Anything that loads late and changes UI can cause layout shifts
%- 3rd party scripts: Need additional DNS lookup, should not be single point of failure


% https://developer.mozilla.org/en-US/docs/Web/Performance/Critical_rendering_path
%- Optimizing for CRP


% https://medium.com/@luisvieira_gmr/understanding-the-critical-rendering-path-rendering-pages-in-1-second-735c6e45b47a
%- Optimizing



% Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
% And slides https://www.igvita.com/slides/2013/fluent-perfcourse.pdf
%- Async all the things! nice image about async attribute
%- Optimizing DOM:
	%- Minify HTML
	%- Compression
	%- Cache in Browser
%- Unblocking CSS:
	%- Media queries: for responsive design
	%-> Move media queries to separate file
	%- <link rel="stylesheet" href="style-print.css" media="print">
	%-> Will not block rendering
%- Optimizing JS:
	%- Minify, compress, cache
	%- JS is parser blocking
	%- Script tag blocks DOM construction
	%- For external JS, browser waits until JS is fetched and executed
	%- CSS blocks rendering and JS execution
	%- Load and execute script after page is loaded
	%- Page is loaded: Browser fires onload event
	%- Async attribute:
		%- <script src="a.js" async></script>
		%- Does not block CRP (DOM construction, CSSOM)
	%- Inline script blocks CSSOM unless included before CSS request
%- General Strategies:
	%- Minify, compress, cache (HTML, CSS, JS)
%	- Minimize use of render blocking resources (CSS):
	%	- Media queries
		%- Inline CSS
%	- Minimize use of parser blocking resources (JS):
	%	- Defer JS execution
		%- async attribute
%	-> Minimize Bytes
	%-> Reduce critical resources
	%-> Shorten CRP length
	
	
% 2013 Grigorik
%- Browser optimizations...

% 2016 Witt code talks
%- tools available:
%- Profiling: GTMetrix, WebPageTest, PageSpeed Insigths
%- Inlining and Optimization: Critical, PostCSS, processhtml
%- Minification and Compression: Goole Closure, tinyPng, Uglifycss and cssmin




% 2014 Hogan https://designingforperformance.com/
% Chapter 2 The Basics of Page Speed - How Browsers Render Content:
%- Browsers try to parallelize requests for content
%- Requests: Optimizing size and amount of requests has big impact on performance, e.g. get all images in one requests using Sprites

%Page Weight is somewhat important:
%- Sum of all file sizes
%- Averages in httparchive, which i also used in one approach % https://httparchive.org/reports/state-of-the-web?start=latest

%Other Impacts on Page Speed
%- "environmental factors"
%- Geography, CDNs
%- Network
%- Browser


% 2016 Witt code talks
%- Possible Improvements:
%- HTTP2
%- Avoid redirects
%- Caching headers
%- CDNs
%- Single Page Apps





%TODO add this ??
% [JavaScript Parsing]
% https://medium.com/reloading/javascript-start-up-performance-69200f43b201






% ---------------------------------------------------------------------------------------------------------------------------------------
% ---------------------------------------------------------------------------------------------------------------------------------------
% ---------------------------------------------------------------------------------------------------------------------------------------
% ---------------------------------------------------------------------------------------------------------------------------------------




\section{Measurement Methods}

%TODO add some more info for Roter Faden, why is this here? whats all about ?
% start with web analyitcs and web performance context
% as described in section web analytics, measurement is important... will be described here
% i could also motivate here that i will use both methods in my experiment thats why i need to explain them here

% [Introduction]

At this point we have a understanding of how web sites are being loaded into the browser and displayed to the user.
Some of the steps are more important for performance than others.
In this section, I will describe how to measure the performance of a web site.

% Why web performance? As we saw its important for business

Multiple methods exists.
The prominent ones are synthetic monitoring and real user monitoring.
They will be discussed below.

Some other methods are mentioned in the last section.

After discussing measurement methods, I can finally discuss the metrics we want to measure.







% ----------------------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------------------



\subsection{Synthetic Monitoring}


\subsubsection{The "Synthetic" Aspect in Synthetic Monitoring}

%\paragraph{Laboratorial Environment}

As the name already suggests, synthetic monitoring is a measurement method performed in an artificial, laboratory-like, synthetic environment.
Test agents simulate real users and are configured to run a browser, load the web site under observation while capturing performance data.
Synthetic monitoring does not take real user traffic into account. % cite 2015 cito

Performance data can be captured using common performance APIs as described in section X.
Additionally, through video recording and analysis,  user centric metrics such as Speed Index can be computed (see section X.) % cite 2021 Wolle 

In synthetic monitoring, many possible configurations and variables of the test agent (client) are under control, such as the location (geography), network conditions, device type, browser version, and so on. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/Rum-vs-Synthetic
Hence, the tester has control over many variables that impact performance.

The controlled environment makes it possible to capture performance data for a specific set up of configurations, such as the test agents location or browser version, which may help to identify issues regarding certain user segments.  For example., a test could check the performance of all users using Firefox in macOS in Germany. % cite 2009 Croll

Apart from defining the "infrastructure" configuration, the tester can also define artificial user journeys to simulate real user behaviour. % cite 2021 Wolle

A characteristic of the controlled environment is that measured data and test results are rather consistent with low variability and can therefore provide a performance base line for the web site under observation and facilitate performance tuning.% cite 2013 Meenan


\paragraph{Synthetic Monitoring is Not Real Users}

Synthetic monitoring does not capture data of real users as the web sites traffic is generated artificially.
Real user behaviour is approximated through simulation by for example predefining user paths.
The measured performance data does not necessarily reflect actual real user experience and the tester should not assume that "synthetic results are like real-user metrics". %cite 2016 Viscomi

Capturing the wide variety and diversity of real-world users such as which pages they visits,, the general configuration of the users machine such as the CPU, GPU and memory performance, what data stores the browser cache and which browser version is being used, the screen size, the operating system, and the network connection to name a few, is difficult to represent in synthetic monitoring. % cite 2013 Meenan, 2013 Grigorik
The selected test configuration in synthetic monitoring only reflects one special use case and can only approximate what a user with a similar set up may experience. % cite 2016 Viscomi
In short, synthetic monitoring test results "are synthetic and therefore not representative for actual user data".  % cite 2021 Wolle



\subsubsection{The "Monitoring" Aspect in Synthetic Monitoring}


Synthetic monitoring can be automated and used to monitor a systems performance in real time while generating up to date reports for the systems maintainer. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/Rum-vs-Synthetic
Monitoring enables to check the availability of the web site around the globe, %cite 2009 Croll
to identify performance issues before real users are aware of them. % cite 2013 Grigorik https://hpbn.co/primer-on-web-performance/
and is in general helpful for continuous "health checks" of the running system. %cite  2021 Wolle 

As any web site can be tested synthetically, it is possible to compare performance data across multiple competitors. % cite 2009 Croll


%TODO this does not really fit here
% [Tools]

Many synthetic monitoring tools exists (For a list of tools, see for example 2016 Kaur or some online resources here).
WebPageTest is one of them and will be discussed in section X.


% [Transition]

Coming back to e-commerce context, as discussed in section X.  a online shops performance correlates with the revenue.
Synthetic monitoring allows to capture performance metrics independent of real user behaviour.
Real user behaviour is not measured in synthetic monitoring.
As only real users are capable of generating revenue, synthetic monitoring can not identify correlations between user satisfaction and performance (as described in section X.).%cite  2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003

In order to do this, RUM is needed.
Real-User Monitoring (RUM) enables to capture data of each individual real user .
RUM will be discussed next.



%TODO cut out google lighthouse completely ?
%- Google Lighthouse


%TODO add waterfall ?
% 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
%- waterfall diagram contains timing information on when the individual resources were requested, from which domain and over what kind of connection each of them was served, and how long transmission took



% 2016 Viscomi
%- synthetic tools are deliberately designed to focus on the performance of a web page under strict conditions that are otherwise highly volatile in real-user performance



% 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
%- Modern tooling also tells you when the browser was actually doing useful work (e.g. rendering) and when it was idle and waiting for loads to finish




% ----------------------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------------------



\subsection{Real-User Monitoring}


%\subsubsection{Measurements with Real Users}

As the name suggests, Real-User Monitoring (RUM) is about collecting and measuring data from real users visiting the web site.
As opposed to synthetic monitoring, where web site traffic is generated artificially and performance experience from real users is only approximated, RUM data relies on real user traffic and captures data directly from each users browser. 
RUM measures the performance as experienced by the users. % cite https://developer.mozilla.org/en-US/docs/Web/Performance/Rum-vs-Synthetic




\subsubsection{The Page Tagging Technique in RUM}

% [Page Tagging]

As already described in section X. Page Tagging is technique to instrument the users browser in order to collect data and report it back to an analytics server.
RUM is based on page tagging, in terms of that it relies on a JS code snippet (tracking or code) which will be loaded into the users browser.
Once this JS code is loaded and executed in the browser, it collects data and sends it back to an analytics service.
If the user blocks JS, or the script can not be downloaded due to other reasons,  RUM will not work.
Once the data arrives at an analytics service, is has to be stored and an interface for the analyst has to be provided in order that he can query the data and get insights, for example by providing a dashboard. %cite 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003

As RUM relies on the JS code, the very first opportunity to measure data is when this JS code has been downloaded and executed.
Anything what happens before this step is not visible for the tracking script.
Meenan states that approximately 20 \% of the loading process lies outside of the RUM measurement scope and "getting a reliable start time for measurements from the real world has been the biggest barrier to using RUM".% cite 2013 Meenan

Another facet of RUM is that ideally the measuring of data has as little as possible impact on the web sites rendering process and that network capacity should not be occupied by RUM scripts and block resources of the CRP (see section X.) % cite  2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
If RUM as a page tagging technique is slowing down the web site under observation is one research question of this thesis.
The evaluation of the controlled experiment tackling this questions states that RUM...., as discussed in great detail in section X. %TODO add conclusion of evaluation here


% [Diversity]

RUM is independent of the users set up or environment and collects data for all active users:
Regardless of the device, the browser,the network condition or the geographical location of the user, as long as the measurement script is downloaded into the users browser, RUM collects data.% cite https://developer.mozilla.org/en-US/docs/Web/Performance/Rum-vs-Synthetic
Hence, RUM data represents each individual user experience. % cite 2016 Viscomi

Through the diversity of users and the unique environment of each user,  RUM data tends to be more diverse and heterogeneous than data collected by synthetic monitoring. % cite 2013 Meenan



\subsubsection{Measure Real User Behaviour}

%TODO here i need to introduce maybe in e-commerce again? Or what is the roter faden? like funnel analysis all this stuff reference it here?

% [E-Commerce Background]

As discussed in section X, a web sites performance and user satisfaction are directly correlated.
A critical part of RUM is to not only capture performance metrics, but also measure user behaviour, for example how the user interacts with the web site and where he clicks. %cite 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
In an e-commerce context, user behaviour questions of interest are for example if a new campaign changes user behaviour as expected or where users leave the check out process. %cite 2021 wolle and % https://developer.mozilla.org/en-US/docs/Web/Performance/Rum-vs-Synthetic

RUM enables the combination of performance metrics with user behaviour and business metrics (see section X.) and can answer questions such as if and how the performance of the web site affects the user behaviour, for example if users buy more or less depending on the web sites speed.% cite Eggplant whitepaper
Thus RUM is not only important for understanding user behaviour, but also for optimizing the web site and to increase revenue.
With techniques such as cookies (see section X.), it is also possible to track user behaviour not only for one page load but over a series of web site visits, leading to even more detailed insights about the visitor. %cite 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003


% [Tools]

Multiple RUM tools and JS libraries exists, such as Boomerang by Akamai. \footnote{\url{https://github.com/akamai/boomerang} [23.06.2021]}
The main player Google Analytics will be discussed in greater detail in section X.


%SpeedKit ?


%TODO add this here?
% [Performance Data]

%Which data and metrics RUM can collect and measure is discussed in greater detail in section X.
%A part from metrics exposed by Web APIs or own implementations
%RUM can also gather data about the users device, operatins system or geographic location. %cite 2015 Cito
%The performance data and timings are exposed through a variety of APIs which the JS tracking code has access to.

% 2013 Grigorik https://hpbn.co/primer-on-web-performance/
- APIs to measure real users (will be discussed in metrics chapter):
- Navigation Timing
- Resource TIming
- User TIming
- The combination of Navigation, Resource, and User timing APIs provides all the necessary tools to instrument and conduct real-user performance measurement for every web application

% Cito
- Real-User Monitoring leverages browser APIs to collect data specific to each end-user transaction

% 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
- Collected information includes various timers to capture network and rendering performance
- To also capture details about the user’s device and browser or on the referring website over which the user arrived, the user agent string and other data artifacts are collected along with the values obtained from the Navigation and Performance Timing APIs


% [Business Data]
conversion rates etc.










% ----------------------------------------------------------------------------------------------



\subsubsection{Chrome User Experience Report (CrUX)}


The Chrome User Experience Report (CrUX) is a RUM method implemented by Google which collects real user data of Chrome users.
As soon as the Chrome user gives his consent, data collection can start and does not need any more set up.

The CrUX only captures data from Chrome users and is therefore not an exhaustive sample of the web users population, as data by users browsing the web with for example Firefox or Safari is not collected.
% cite 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003

The collected data is available via Googles PageSpeed Insights, the CrUX Dashboard or the BigQuery and CrUX API. % cite https://developers.google.com/web/tools/chrome-user-experience-report


%TODO add here which metrics it collects?



% ----------------------------------------------------------------------------------------------



%TODO even add this ?
\subsection{Log Files and Surveys}

Other RUM methods are surveys and log file analysis.


% 2021 Wolle blog post https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003

Log analysis:
- concerned with technical performance in the backend
- generates insights from the data that is already available from the application server, proxy, or content delivery network (CDN)
- TTFB
- Server logs typically only reflect the time it took to send the first response, not until the client actually started receiving it
- Analyzing logs from application servers, CDNs, or proxies is a reasonable first step to discover potential bottlenecks, but it only covers the server side and does not provide any information on client-side processing in general or rendering performance in particular


Surveys:
- directly ask the users for their opinions
- direct approach to understanding whether or not users are satisfied with website performance: Just ask them for their opinion
- online surveys or by offering some sort of price or chance to win a competition in exchange for the users opinions. 
- Other options include actual interviews or monitoring users in a lab setting to find out how they react while surfing on the website.
- they only cover a small sample of the user base and therefore may be subject to a certain selection bias
- user perception can be highly inaccurate
- Getting reliable info from user surveys is therefore no trivial task
- some forms of surveys (e.g. lab experiments) can be relatively expensive to conduct in comparison to some of the fully automated alternatives for collecting information
- are great for collecting qualitative feedback on the user experience, but are not suitable for gathering quantitative measurements




% ----------------------------------------------------------------------------------------------



\subsection{Measurement Methods Conclusion}


Multiple methods to measure performance of a web site exist.
Two main complementary techniques exist, Synthetic Monitoring and Real-User Monitoring.

Synthetic Monitoring measures a web sites performance in a controlled environment using test agents and is especially useful to find a performance base line of the web site and for continuous monitoring and health checks.
Performance as end users may experience it can only be approximated it is not captured by synthetic monitoring.

RUM collects data from each user visiting the web site and reports it back to an analytics server.
RUM is especially useful when combining multiple metrics together such as performance and business metrics in order to analyse user behaviour.
On the other hand, when no user is visiting the web site, RUM is not collecting any data.

Other performance measurement methods such as CrUX and surveys provide browser specific or quality data and complement the analysts tool box.


%TODO add some comparison table?


% [Metrics, Transition]

Metrics are critical in order to map performance to some sort of value or number.
Through metrics, performance is quantifiable to some extent and therefore comparable.

As already described, measurement methods such as synthetic monitoring or RUM can measure metrics such as "performance" or "business" metrics.
What are does metrics exactly?
What do they reflect?
How can they be measured?

Those questions will be addressed in the next section.





% --------------------------------------------------------------------------------------------------------------------------------------------
% --------------------------------------------------------------------------------------------------------------------------------------------




\section{Metrics}


%TODO roter faden: motivate with context web analytics, web performance, user satisfaction etc

%TODO ideally come up with a nice table which abstracts and summarizes everything about metrics:
% e.g. measured by: browser, server. Technical stuff is measured in browser, everything else not in browser
% technical, UX, ...


% Data vs Metric ?

Introduction: What are metrics, Different kind of metrics
General, web analytics (Business) Metrics and then take a look at performance metrics
Page weight, Performance, UX and custom Metrics
Metrics Taxonomy


% [Quantifiable]

reflect in numbers
numbers can be compared: over time, after change, with competitor, etc.

% https://developer.mozilla.org/en-US/docs/Learn/Performance/Measuring_performance
- Compare to competitors
- Compare different versions of your app

% 2009 Jansen
- use these metrics to analyze Web traffic and improve a Website to meet better the expectations of the site’s traffic

% 2014 Singal:
"Metrics serve as the basic information for analyzing Web traffic and helps in improving a Website to meet its goals"


% [Relevance]

- Measure what is important for you
tailored, best would be to have custom metrics, see section X.

% https://developer.mozilla.org/en-US/docs/Learn/Performance/Measuring_performance
- Metrics should be relevant to your users, site, and business goals

% 2009 Croll p 7
- each business has its own definitions of success
- p.15 "whether your business benefited in some way from their visits."
- search engine vs e-commerce website example



% [Usable]

% https://developer.mozilla.org/en-US/docs/Learn/Performance/Measuring_performance
- should be collected and measured in a consistent manner
- analyzed in a format that can be consumed and understood by non-technical stakeholders

% 2009 Waisberg 2012 Kumar
 Good metrics should be: Uncomplex, Relevant, Timely, Instantly Useful
 

% 2019 Enghardt
"metrics as well as experiments have to realistically reflect possible performance improvements for actual users"




%TODO connection to KPIs?
% 2015 Bekavac



\subsection{Business or "Non-Performance", general web analytics Metrics}

Unlimited metrics, also when thinking about custom metrics.
The main question here is how to categorize those metrics.
Best is: numerical values, Relations, non-numerical values


\subsubsection{Possible Categorizations of Web Analytics Metrics}


Semantic categorization
Orthogonal: "number" categorization: Count, ratio, ...


% Wolle https://www.youtube.com/watch?v=avPcOFzUa1Q&ab_channel=WolframWingerath
- User Engagement:
	- Session Length
	- Time on site
	- First user interaction
	- Bounce rate

- Business KPIs:
	- Cart size
	- Transactions
	- Conversion Rate
	- Revenue

- QA Metadata:
	- Page views and sessions
	- Browser distribution
	- JavaScript errors
	- Caching insights
	
-> Semantic, about the meaning of metrics
- 
	
	
% 2004 Peterson
- Argument with Customer Life Cycle: Retention - Awareness - Conversion

 Measuring Reach:
 - Overall Traffic Volumes
 - Number of Visits
- Number of New Visitors
- Percentage of New Visitors
-...

 Measuring Acquisition:
- Percent New Visitors
- Average Number of Visits per Visitor
- Average Number of Page Views per Visit
- Average Pages Viewed per Visitor
-...
 
 Measuring Conversion:
 - Conversion Rates
- Abandonment Rates
- ... 
 
 
 Measuring Retention:
 - Number of Returning Visitors
 - ...
 
-> customer-related, customer-centric
- Marketing


% 2007 Burby
- WAA
- 3 Types of web analytics metrics:
- Counts: single number
- Rations: Count divided by count
- KPIs: count or ration infused by business strategy

- Counts and ratios are categories for number types
- KPIs bring the dimension of business, give semantic meaning to numbers


% 2009 Jansen p.30
 4 categories: site usage, referrers, site content analysis, quality assurance
 8 fundamental metrics
 Site usage:
	 Demographics and System Statistics
	 Internal Search Information
	 Visit Length
	 Visitor Type	
 Referrers:
	 Referrering URL and Keyword Analysis	
 Site content analysis:
	 Top Pages
	 Visitor Path
 Quality assurance:
	 Errors
	 
	 
% 2009 Croll
All metrics answer one of the Big 4 Question:
- What did they do?
- How did they do it?
- Why did they do it?
- Could they do it?

- Very user centric



% 2015 Bekavac
- Metrics for describing visits: entry page, landing page, exit page, visit duration, referrer, ctr
- For describing Visitors: new visitors, returning visitor, repeat visitor, visits per visitor, recency, frequency
- For describing Visitor engagement: page exit ratio, bounce rate, page views per visitor
- Conversion metrics: conversion, conversion rate



% 2017 Hassler
 Types: Anzahl, Relations, Werte
 
Counts:
- Absolut value
- visitors
- sales total
etc

Relations:
- Put aboslute metrics in relation
- As Relation or Percentage
- Most important type
- Page Views per Visitor

Values:
- Referrer
- Search term 
 

I will use this categorization to list some metrics.



% 2020 Heinemann 4.1.4
Metrics Categories for E-Shop:
- Attraction
- Acquisition
- Retention
- Umsatzleistung
- Warenleistung
- Ergebnisleitung

- also customer life cycle






% [Conclusion]

We have seen that multiple categorizations of metrics exists.
I will now use possible categorization: Counts, Ratios, Values
For each category, i will list some metrics and explain them or even put it in table.
This list is not complete.

After that, i will take a look at metrics which are specifically for performance.



% Customer Life Cycle and Pyramid
% 2004 Peterson
%Pyramid Model described the data itself and not metrics
%Metrics categorised by customer life cycle: Reach, Acquisition, Conversion, Retention

% 2008 Reese
%- Pyramidenmodell 




% ---------------------------------------------------------------


\subsubsection{Counts and Numerical Values}


Counts describe how many
Can be measured Over time: last hour, month, from to, etc.
Averages are possible
Compare with Segmentation


%TODO maybe put this in table
% background color for titles

\begin{center}
	\small
	\begin{longtable}{ | p{0.3\linewidth} | p{0.7\linewidth} | }
	
	\multicolumn{2}{|c|}{Counts} \\
	\hline
	Hits & blablabla \\
	\hline
	Visits & blabla \\
	\hline
	\multicolumn{2}{|c|}{Ratios} \\
	\hline
	Conversion Rate & blbla \\
	
	\caption{Your caption here} % needs to go inside longtable environment
	\label{tab:businessmetrics}
	\end{longtable}
\end{center}








\paragraph{Hits}
% 2004 Peterson
% 2017 Hassler


\paragraph{Visits}
% 2004 Peterson
% 2009 Waisberg
% 2017 Hassler

\paragraph{Sessions}
% 2007 Burby


\paragraph{Session Length}
% 2007 Burby


\paragraph{Page Views}
% 2004 Peterson
% 2007 Burby

% 2009 Croll
p. 74 Page View, first useful web analytics metric

% 2009 Waisberg

% 2015 Zheng

% 2017 Hassler


\paragraph{Single Page Visits}
% 2007 Burby

\paragraph{New Visitors}
% 2004 Peterson
% 2007 Burby

\paragraph{Unique Visitors}
% 2004 Peterson
% 2007 Burby
% 2017 Hassler

\paragraph{Repeat Visitors}
% 2007 Burby

\paragraph{Return Visitor}
% 2007 Burby

\paragraph{Time on Page}
\paragraph{Time between Visits}

\paragraph{JS Errors}

\paragraph{Cart Size}
% 2009 Croll p 24 in site effectiveness


\paragraph{Conversions}


many more

% ---------------------------------------------------------------------------------




\subsubsection{Ratios}

Combining numerical values
As Ratio or Percentage

\paragraph{Bounce Rate}
% 2007 Burby
% 2009 Waisberg

\paragraph{Conversion Rate}
% 2004 Peterson

% 2009 Croll
- The percentage of visitors that your site converts to contributors, buyers, or users is the most important metric you can track

\paragraph{Abandonment Rate}
% 2004 Peterson
% 2009 Croll

\paragraph{Page Views per Visitor}
% 2007 Burby
% 2009 Waisberg

\paragraph{New Visitors Percentage}
% 2009 Waisberg


\paragraph{Visits per Visitor}

\paragraph{Page Exit Ratio}
% 2007 Burby


\paragraph{Click Through Rate (CTR)}
% 2004 Peterson
% 2007 Burby
% 2009 Croll


many more


% ---------------------------------------------------------------------------------




\subsubsection{Non-Numerical Values}

Can not be reflected as number

%TODO find some more examples here

\paragraph{Demographics}
age, gender, browser, geography, ...

\paragraph{Referrer}
% 2004 Peterson
% 2009 Croll


many more






% [Transition]

List is not complete.
Now we move to performance specific metrics.
As performance is directly connected to time, most of those performance metrics are measuring a timestamp.
Some are also displaying a score. 
This will be discussed.



%TODO should i show how to actual measure those metrics, like visits, hits, conversion rate etc ?




% Kessler 2012 -> dont have this digital...
% Erfolg messen und bewerten
 %Traffic:
%	 Page Impression / Page View
%	 Visit
%	 Visitor / Unique visitor
% Bounce rate
 %Conversion rate
 %CTR: Click-through-rate
 %Session length
 
 
 % 2016 Kollewe -> nicht digital
% Besucheranalyse: Wie viele Besucher?, Anzahl Besucher mit Mobilgerät, Demographische Daten (Geschlecht, Altersgruppe)
% Seitenanalyse: Was machen die Besucher im Shop?, Zielseite / Startseite: Erste Seite, die ein Besucher angeschaut hat, Ausstiegseite
% E-Commerce-Analyse: Transkations-daten aus Shop, Funnel-Analyse



% --------------------------------------------------------------------------------------------------------------------------------------------
% --------------------------------------------------------------------------------------------------------------------------------------------



\subsection{Performance Metrics}


% [Categories]

As with general web analytics metrics, there are multiple ways to categorize performance metrics.

% 2019 Enghardt
- network-centric, browser-centric, and user-centric metrics

For me most intuitive is

Metrics regarding Page Weight and size of the web site
Timing Metrics, how long each step from the CRP took
UX Metrics, try to reflect how users experience performance, visual metrics
Custom Metrics




% -----------------------------------------------------------------------



\subsubsection{Page Weight}

- the size of the web site
- Page weight has an impact on performance
- Page consists of resources
- More resources and bytes means more downloads and waiting time


% HTTP Archive
- Generally for all resources: Bytes and Requests
- Total Size: Number of bytes sent over the network, which may be compressed

- Total Kilobytes
- Total Requests

- HTML Bytes
- HTML Requests

- CSS Bytes
- CSS Requests

- JavaScript Bytes
- JavaScript Requests

- Font Bytes
- Font Requests

- Image Bytes
- Image Requests

- Video Bytes
- Video Requests

- Other Bytes
- Other Requests


% -----------------------------------------------------------------------


\subsubsection{Performance Timings Metrics}


\paragraph{Web Standards}

% https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/The_web_and_web_standards
- in 1994, TimBL founded the World Wide Web Consortium (W3C), an organization that brings together representatives from many different technology companies to work together on the creation of web technology specifications

Web Standards:
- technologies we use to build web sites
- standards exist as long technical documents called specifications
- detail exactly how the technology should work
-  to be used by software engineers to implement these technologies (usually in web browsers)
- For example, the HTML Living Standard describes exactly how HTML (all the HTML elements, and their associated APIs, and other surrounding technologies) should be implemented.
- Others  WHATWG (who maintain the living standards for the HTML language), ECMA



\paragraph{W3C}


What is W3C

% https://www.w3.org/standards/faq
W3C publishes documents that define Web technologies. These documents follow a process designed to promote consensus, fairness, public accountability, and quality. At the end of this process, W3C publishes Recommendations, which are considered Web standards.

e.g.DOM,  HTML, SVG, ...



% [Recommendation Process]

% https://www.w3.org/2020/Process-20200915/#rec-track
- Recommendation track:
Publication of the First Public Working Draft.
Publication of zero or more revised Working Drafts.
Publication of one or more Candidate Recommendations.
Publication of a Proposed Recommendation.
Publication as a W3C Recommendation.

- From Idea to concrete API which browser maintainers implement

% maturity levels https://www.w3.org/2015/Process-20150901/#maturity-levels
- Working Draft (WD)
- Candidate Recommendation (CR)
- Proposed Recommendation
- W3C Recommendation (REC)





\paragraph{Web Performance Working Group}


% [Introduction]

Multiple groups exists which are responsible for a specific topic.
Performance working group is responsible for performance.

% 2013 Meenan
- In 2010, the browser vendors got together under the W3C banner and formed the Web Performance Working Group to standardize the interfaces and work toward improving the state of Web-performance measurement and APIs in the browsers
- Clock issue -> High Resolution Time specification

% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- WebPerf Working Group (founded 2010)
- "Provide methods to measure aspects of application performance of user agent features and APIs"

% https://www.w3.org/2021/02/webperf.html
- Working Groups push new recommendations and invent them
Web Performance Working Group is responsible:
- Measurement
- Scheduling
- Adaptation
-> Important is Measurement


% https://w3c.github.io/perf-timing-primer/
- APIs used to create waterfall charts


% [Specs]

Normative Specifications:
- High Resolution Time
- Performance Timeline
- Resource Timing
- Navigation Timing
- User Timing
- Page Visibility
- Long Task
- Paint Timing


Server Timing:
% https://www.w3.org/TR/server-timing/

% https://w3c.github.io/perf-timing-primer/#dfn-time-origin
- attempt to address the concern about lacking insight into how or why certain stages of the request-response cycle have taken as much time as they have


others, not important for metrics:
- Beacon
- Cooperative Scheduling of Background Tasks
- Device Memory
- Reporting
- Network Error Logging


% and some unofficial drafts
% Network Information API (not official w3c) https://wicg.github.io/netinfo/
% Element Timing API (not official w3c) https://wicg.github.io/element-timing/
% Event Timing API (not official w3c) https://wicg.github.io/event-timing/


% https://developers.google.com/web/fundamentals/performance/navigation-and-resource-timing
- Navigation and Resource Timing (as well as other related APIs) store performance entries in a performance entry buffer
- they store performance metrics for pages and resources into a list accessible by JavaScript
- methods exist in the window.performance namespace
-  getEntriesByType method
- Any time you've seen a waterfall chart, you're looking at a visual representation of the data these APIs provide




Now i will take a closer look at Navigation, Resource and User Timing and which metrics are derived from there.





% -----------------------------------


\paragraph{High Resolution Time}


% [Date/Clock Issue]


% https://www.w3.org/TR/hr-time-3/
- Old: Date from ECAMScript: Date object as a time value representing time in milliseconds since 01 January, 1970 UTC
-  DOMTimeStamp is defined similarly
- definitions of time are subject to both clock skew and adjustment of the system clock
- The value of time may not always be monotonically increasing and subsequent values may either decrease or remain the same.
- resolve issues by providing monotonically increasing time values with sub-millisecond resolution

% 2013 Meenan
- Clock issue -> High Resolution Time specification

% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- Non-Decreasing
- Earlier: Date object

% https://w3c.github.io/perf-timing-primer/
- sub-millisecond resolution
- provide a monotonic, uniformly increasing timestamp suitable for interval measurements


% https://developers.google.com/web/updates/2012/08/When-milliseconds-are-not-enough-performance-now
- performance.now() is a measurement of floating point milliseconds since that particular page started to load
-> the performance.timing.navigationStart timeStamp to be specific
- number stays relative to the page because you'll be comparing two or more measurements against eachother


% [Time Origin]

% https://www.w3.org/TR/hr-time-3/




% [DOMHighResTimeStamp]

% https://www.w3.org/TR/hr-time-3/
-  used to store a time value in milliseconds, measured relative from the time origin, shared monotonic clock, or a time value that represents a duration between two DOMHighResTimeStamps


% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- Start time: navigation start
- Get time with: performance.now()
	
	
% https://developer.mozilla.org/en-US/docs/Web/API/DOMHighResTimeStamp
- value of a DOMHighResTimeStamp is a double-precision floating-point number which describes the number of milliseconds (accurate to within 5 microseconds if the device supports it) elapsed between two points in time



% [Performance Interface]

% https://www.w3.org/TR/hr-time-3/
- now()
- timeOrigin

% Measuring Performance https://developer.mozilla.org/en-US/docs/Learn/Performance/Measuring_performance
Performance API includes:
- Performance Timeline API, the Navigation Timing API, the User Timing API, and the Resource Timing API.


% https://developer.mozilla.org/en-US/docs/Web/API/Performance
- provides access to performance-related information for the current page
- part of the High Resolution Time API, but is enhanced by the Performance Timeline API, the Navigation Timing API, the User Timing API, and the Resource Timing API
- can be obtained by calling the window.performance


% https://developer.mozilla.org/en-US/docs/Web/API/Performance_API
Performance API 
-> Includes Performance Timeline API, the Navigation Timing API, the User Timing API, and the Resource Timing API.


% https://developer.mozilla.org/en-US/docs/Web/API/Performance_API/Using_the_Performance_API
- Performance: base interface for these standards
- methods and properties are extended by different standards







% -----------------------------------

%TODO move this down ?
Is this performance timeline ?

\paragraph{Performance Object}


% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
=> Window.performance returns Performance object
=> Performance object exposes:
	- High Resolution Time API
	- Navigation Timing API
	- Resource Timing API
	- Performance Timeline API
	- User Timing API
	
	





% -----------------------------------


\paragraph{Navigation Timing}

- If possible make one deep dive into one specification: What exactly gets measured? Maybe check out html standard, v8 or chromium implementation, etc.
- Version 1 and 2


% https://w3c.github.io/perf-timing-primer/
Navigation Timing:
- access the complete timing information for navigation of a document
- Navigation is about how user agents convert the requested HTML, CSS, and JavaScript into rendered pixels, which is one of the most critical steps for users to navigate a document



% [Level 1]


% https://www.w3.org/TR/navigation-timing/
- This specification defines an interface for web applications to access timing information related to navigation and elements.
- complete client-side latency measurements
- PerformanceTiming Interface: ....
- PerformanceNavigation interface
- Both are collected in Performance Interface
- Performance Interface is accessible through window.performance

- time is measured in milliseconds since midnight of January 1, 1970 (UTC).
- Navigation Timing 2 specification allows access to timing information related to navigation using sub-milliseconds resolution instead


% https://www.w3.org/TR/navigation-timing-2/
- kept for backwards compatibility
- PerformanceTiming Interface


% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings
- Navigation timings are metrics measuring a browser's document navigation events
- measures the main document's timings
- read only
- deprecated in favour of the Performance Entry API -> level 2
- supported in all browsers

- PerformanceTiming API (this is the same as PerformanceTiming Interface) %https://developer.mozilla.org/en-US/docs/Web/API/PerformanceTiming
- Navigation process

- Metrics which are calculated from this:
- time to first byte
- page load time
- dns lookup
- whether the connection is secure.



% [Level 2]

% https://www.w3.org/TR/navigation-timing-2/
- Replaces first version of Navigation Timing
- See summary of improvements
- Builds on top of Resource Timing 2
- specification defines the PerformanceNavigationTiming interface
- Uses high resolution time 2
- Processing Model (image with all attributes defined by PerformanceNavigationTiming interface)



% https://developer.mozilla.org/en-US/docs/Web/API/PerformanceNavigationTiming
PerformanceNavigationTiming Interface: 
- extends PerformanceEntry Interface
- extends PerformanceResourceTiming Interface


% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings
- measuring the performance of the main page, generally the HTML file via which all the other assets are requested

PerformanceEntry API:
- provides for marking and measuring times along the navigation and resource loading process
- You can also create marks
- performance.getEntriesByType('navigation') -> get navigation timings
- performance.getEntriesByType('resource') -> get resource timings


% https://developer.mozilla.org/en-US/docs/Web/API/PerformanceEntry
- The PerformanceEntry object encapsulates a single performance metric that is part of the performance timeline


PerformanceNavigationTiming % https://developer.mozilla.org/en-US/docs/Web/API/PerformanceNavigationTiming





% https://developer.mozilla.org/en-US/docs/Web/API/Navigation_timing_API



% 2013 Grigorik
- navigation timing measures the main document's timings
- The real benefit of Navigation Timing is that it exposes a lot of previously inaccessible data, such as DNS and TCP connect times, with high precision (microsecond timestamps), via a standardized performance.timing object in each browser.
- Hence, the data gathering process is very simple: load the page, grab the timing object from the user’s browser, and beacon it back to your analytics servers!
- By capturing this data, we can observe real-world performance of our applications as seen by real users, on real hardware, and across a wide variety of different networks.
- Navigation Timing provides performance timers for root documents only


Navigation Events: Exposed by Navigation Timing API: navigationStart, domContentLoaded, etc.



% 2013 Meenan
- In late 2010 the group released the Navigation Timing specification
- The largest benefit of navigation timing is that it exposes a lot of timings that lead up to the HTML loading --> This is this famous image
- In addition to providing a good start time, it exposes information about any redirects, DNS lookup times, time to connect to the server, and how long it takes the Web server to respond to the request, for every user and for every page the user visits
- The measurement points are exposed to the DOM (Document Object Model) through the performance object and make it trivial to calculate load times (or arbitrary intervals, really) from JavaScript.



% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- Navigation Timing API:
	- Page loading cycle
	- Exposes different time points
	- Actual loading of the page
	- De facto standard

	- window.performance.timing
	
	- window.performance.navigation -> redirectCount
	- window.performance.navigation.type:
		- 0: navigate
		- 1: reload
		- 2: back / forward

	- Navigation start
	- Redirect
	- Cache
	- DNS
	- TCP
	- Request
	- Response
	- Processing
	- Load Event Start
	- Load Event End

	- e.g. call in onLoad handler (timestamps are set)

	- Part of performance timeline

	- Before Navigation Timing?:
		- beforeUnload, unload, cookies, ...
		
		
		
		
		
% https://developers.google.com/web/fundamentals/performance/navigation-and-resource-timing
DNS lookup:
- domainLookupStart marks when a DNS lookup starts.
- domainLookupEnd marks when a DNS lookup ends.

Connection negotiation:
- connectStart marks when the client opens a connection to the server.
- secureConnectionStart marks when the client begins TLS negotiation.
- connectEnd marks when connection negotiation ends (including TLS time).

Requests and responses:
- fetchStart marks when the browser starts to fetch a resource. This is distinct from a request in that it doesn't mark when the browser makes a network request for a resource, but rather when it begins checking caches (e.g., HTTP and service worker caches) to see if a network request is even necessary.
- workerStart marks when a request is being fetched from a service worker within a fetch event handler (if applicable). This will be always be 0 if a service worker isn't installed for the current page.
- requestStart is when the browser issues the network request.
- responseStart is when the first byte of the response arrives.
- responseEnd is when the last byte of the response arrives.


Document unloading:
- unloadEventStart and unloadEventEnd 

Redirects:
- redirectStart and redirectEnd 

Document processing:
- domInteractive, domContentLoadedEventStart, domContentLoadedEventEnd, and domComplete

Loading:
- loadEventStart and loadEventEnd

Document and resource size:
- transferSize is the total size of the resource including HTTP headers.
- encodedBodySize is the compressed size of the resource excluding HTTP headers.
- decodedBodySize is the decompressed size of the resource (again, excluding HTTP headers).









% [Metrics]

% TTFB

% https://developer.mozilla.org/en-US/docs/Glossary/time_to_first_byte
TTFB = responseStart - navigationStart



% PLT

% https://developer.mozilla.org/en-US/docs/Glossary/Page_load_time
pageloadtime = time.loadEventStart - time.navigationStart;

% 2013 Wang:
- PLT as central metric



% DNS Lookup time

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#dns_lookup_time
dns  = time.domainLookupEnd - time.domainLookupStart;


% https://developers.google.com/web/fundamentals/performance/navigation-and-resource-timing
domainLookupStart marks when a DNS lookup starts.
domainLookupEnd marks when a DNS lookup ends.
var pageNav = performance.getEntriesByType("navigation")[0];
var dnsTime = pageNav.domainLookupEnd - pageNav.domainLookupStart;

- both domainLookupStart and domainLookupEnd (and others) can be 0 for a resource served by a third party if that host doesn't set a proper Timing-Allow-Origin response heade




% TCP

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#tcp
tcp  = time.connectEnd - time.connectStart;



% TLS / SSL

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#ssl_negotiation
ssl = time.requestStart - time.secureConnectionStart;

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#compression
- compression savings percentage

% https://developers.google.com/web/fundamentals/performance/navigation-and-resource-timing
- connection phase consists of three metrics
- connectStart marks when the client opens a connection to the server.
- secureConnectionStart marks when the client begins TLS negotiation.
- connectEnd marks when connection negotiation ends (including TLS time).

- var connectionTime = pageNav.connectEnd - pageNav.connectStart;
- tlsTime = pageNav.connectEnd - pageNav.secureConnectionStart;



% Request Time

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#request_time
request = timing.responseStart - timing.requestStart




% Load Event Duration

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#load_event_duration
load = timing.loadEventEnd - timing.loadEventStart 




% DOM Content Loaded Event

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#domcontentloaded_event
DOMContentLoaded = timing.domContentLoadedEventEnd - timing.domContentLoadedEventStart



% Duration

% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings#duration
duration = PerformanceNavigationTiming.loadEventEnd - PerformanceEntry.startTime






% https://developers.google.com/web/fundamentals/performance/navigation-and-resource-timing


% https://community.akamai.com/customers/s/article/Using-Navigation-Timing-APIs-to-understand-your-webpage?language=en_US



% -------------------


\paragraph{Resource Timing}


% https://www.w3.org/TR/resource-timing-2/




% https://w3c.github.io/perf-timing-primer/
Resource Timing:
- set of attributes for each loaded resource
- resources usually refer to HTML documents, XHR objects, links (such as a stylesheet) or SVG elements
- access a set of critical network timing attributes for each resource on the page
- performance.getEntriesByType("resource"): array of Resource Timing Objects for each requested resource
- The PerformanceResourceTiming interface extends the PerformanceEntry interface in the Performance Timeline
- Each of these timestamps is in microseconds, which are provided by the window.performance.now() method in the High Resolution Time specification.



% https://developer.mozilla.org/en-US/docs/Web/Performance/Navigation_and_resource_timings
- resource timing measures timing for individual resources, the assets called in by the main page, and any assets that those resources request
- Many of the measurements are similar to navigation timing


% https://developer.mozilla.org/en-US/docs/Web/API/PerformanceResourceTiming
- retrieval and analysis of detailed network timing data regarding the loading of an application's resources
- determine, for example, the length of time it takes to fetch a specific resource like request, svg, image, script



% https://developer.mozilla.org/en-US/docs/Web/API/Resource_Timing_API


% https://developer.mozilla.org/en-US/docs/Web/API/Resource_Timing_API/Using_the_Resource_Timing_API



% 2013 Grigork
- Resource Timing provides similar performance data for each resource on the page, allowing us to gather the full performance profile of the page.
- resource timing provides the times for all the assets or resources called in by that main document and the resources' requested resources.



% 2013 Meenan
- exposes timing information about every network request the browser had to make to load a page and what triggered each request (whether stylesheet, script, or image)


% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- Resource Timing API:
	- Similar to Navigation Timing API
	- Measuring all resources on the page
	- Inspiration: DevTools Waterfall diagram

	- List of all resources fetched

	- Set of timestamps for every single resource fetched
	- getEntriesByType("resource")

	- Buffer: only stores first 150 entries

	- Data:
		- Convert to JSON
		- Compressing
		- HTTP Archive: on average 103 HTTP resources per page -> 45 KB data per page load

	- Timing-Allow-Origin:
		- HTTP Header
		- Cross domain: Any domain that is not the domain where the page is loaded from
		-> Considered as "cross-origin-domain" by browser
		- By default: Resource Timing hides all details for cross origin domains
		- For privacy reasons
		-> Set in HTTP Response header timing-allow-origin

	- Blocking Time:
		- How long the browser is waiting for an open connection
		- Limited number of open connections

	- Cache Hits:
		- Check if resource loaded from cache or other server



% -------------------


\paragraph{User Timing}

% https://w3c.github.io/perf-timing-primer/
User Timing:
-  obtain the high precision performance characteristics of an individual task which is important to you
- unique marks
- PerformanceMeasure interface: calculate the elapsed time between two marks



% 2013 Girgorik
- Similarly, User Timing provides a simple JavaScript API to mark and measure application-specific performance metrics with the help of the same high-resolution timers


% https://developer.mozilla.org/en-US/docs/Web/API/User_Timing_API


% 2013 Meenan
- provides a simple way of marking points in time using performance. mark(“label”) and a standard way of querying all of the custom measurements later


% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- User Timing API:
	- Not a browser API
	- Measure events and durations
	- Keep track of marks (timestamps)
	- Mark: timestamp
	- Measure: delta between two timestamps
	- performance.mark
	- Performance.measure
	-> Puts data in Performance Timeline / DevTools

	-> Create your own timestamps and measures.
	-> Also visible in WebPageTest



% -------------------


\paragraph{Performance Timeline}


% https://w3c.github.io/perf-timing-primer/
Performance Timeline:
-  Performance Timeline APIs was introduced in 2011
- unified interface to obtain various performance metrics
- The Performance Timeline API uses PerformanceEntry.entryType to describe the type of the interface represented by this PerformanceEntry object, which represents performance measurements

- PerformanceEntry object represents performance measurements
- PerformanceEntry object:
	- name
	- entryType
	- startTime
	- duration
	
	
	
Performance Timeline API % https://developer.mozilla.org/en-US/docs/Web/API/Performance_Timeline
Performance Entry API % https://developer.mozilla.org/en-US/docs/Web/API/PerformanceEntry



% [Performance Entry]

% https://developer.mozilla.org/en-US/docs/Web/API/PerformanceEntry
- PerformanceEntry object encapsulates a single performance metric that is part of the performance timeline
- A performance entry can be directly created by making a performance mark or measure

Always of type:
- PerformanceMark
- PerformanceMeasure
- PerformanceFrameTiming
- PerformanceNavigationTiming
- PerformanceResourceTiming
- PerformancePaintTiming

Properties:
- name
- entryType
- startTime
- duration

	
	

Version 1 and 2

- store and retrieve performance metric data
- does not cover individual performance metric interfaces


- defines PerformanceEntry.entryType to describes the type of the interface represented by this PerformanceEntry object
- PerformanceEntry object represents performance measurements


Types values:
- mark (User Timing)
- measure (User Timing)
- navigation (Navigation Timing)
- resource (Resource Timing)
- longtask


% [Level 2]

PerformanceObserver

Performance Observer API % https://developer.mozilla.org/en-US/docs/Web/API/PerformanceObserver


% Measuring Real User Performance in the Browser https://www.youtube.com/watch?v=yrWLi524YLM&ab_channel=NicJansma
- Performance Timeline API:
	- window.performance.getEntries()
	- getEntriesByType(), getEntriesByName()
	- Capturing different performance metrics
	- Browser APIs can add data to this timeline

	- performance.getEntries(): List of events that happened on that page




% ------------------------------------------------------------------------------------------------------------------




\paragraph{Conclusion}


Provide here a table with all important recommendations and the metrics they expose.


% 2013 Girgorik: "The combination of Navigation, Resource, and User timing APIs provides all the necessary tools to instrument and conduct real-user performance measurement for every web application"



% -----------------------------------------------------------------------
% -----------------------------------------------------------------------





\subsubsection{UX / User Perceived, Visual Performance Metrics}


% https://developer.mozilla.org/en-US/docs/Learn/Performance/Perceived_performance



% Web Vitals and Core Web Vitals




\paragraph{Web Vitals}


% https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
a set of browser-based metrics for capturing user-perceived performance. Google heralds the Web Vitals as the gold standard for website performance across a number of services (e.g. PageSpeed Insights, Search Console, TestMySite) and publishes them in the Chrome User Experience Report (CrUX) [7] database. The core metrics will further be used for ranking search results starting in June [6] and even non-AMP pages will be admitted to the Top Stories [4] feature in Search on mobile — given they exhibit top-of-the-line performance according to the Core Web Vitals. Website performance according to the Web Vitals will therefore be critically important for SEO in the upcoming years.



% Walton user-centric
% Walton https://web.dev/vitals/
% Walton user centric metrics


 Key questions: Is is usable, is it delightful, ...
 Types of metrics
 important metrics
 custom metrics


% 2017 Panicker, Walton
 Core Web Vitals: First Input Delay, Cumulative Layout Shift, Largest Contentful Paint
 First Paint, First Contentful Paint: Is it happening? PerformanceObserver
 First Meaningful Paint, Hero Element: Is it useful? 
 Time To Interactive: Is it usable? Use Polyfill
 Long Tasks: Is it delightful? PerformanceObserver
 Total Blocking Time
 Time To First Byte



\paragraph{Core Web Vitals}

% Introduction
% 2020 Sullivan

 Most important metrics, Apply to all websites, Measures real user experience, Measurement support for Lab and Field, Concise and clear
 LCP: Progressive loading. FCP may become a core web vital
 FID: Interactivity during load
 CLS: Visual stability
 Future goals: Better support for Single Page Apps, Input responsiveness, Scrolling and animations
 Areas of user experience beyond performance: Security, Privacy, Accessibility


% LCP https://web.dev/lcp/

 Introduction, what is it
 How to measure
 How to improve
 
 
 Google had some issues with capturing LCP in chrome, see twitter
 Because of this they needed to release a new chrome version
 CruX also has 2 different lcps now (?)
 use this story to show that this stuff is far from trivial


% FID https://web.dev/fid/

 Introduction, what is it
 How to measure
 How to improve


% CLS https://web.dev/cls/
% TODO new stuff https://blog.webpagetest.org/posts/understanding-the-new-cumulative-layout-shift/?utm_medium=email&_hsmi=121601471&_hsenc=p2ANqtz-9IsSdXActEE6lw4BrDZNa4eFqzQZjgabLHbq7aS-c2KkhqLGNtkIaGfQYD4VqZe9_6ZYFlTmlCgB87THSfsnVM1fl7NiixtrJqAsVO6DPUjeJIo6c&utm_content=121601471&utm_source=hs_email

 Introduction, what is it
 How to measure
 How to improve



% Tool support
% Osmani: Tools to measure
% Walton user-centric: How to measure in Lab and Field
% Philip Walton. Best practices for measuring Web Vitals in the field


% Science background
% 2020 Sagoo: Science: Thresholds, impact on business
% McQuade: thresholds



\paragraph{Other UX Metrics}

% Measuring Real User Performance in the Browser youtube:
% - first paint, fcp, fmp, visually complete, speed index



 Visually complete ?


% 2018 Netravali:
% Page load time, TTFP, Above the fold time, Speed Index, User perceived PLT, TTI

% https://docs.webpagetest.org/metrics/speedindex/
\subparagraph{Speed Index}




% 2021 Meenan vimeo


% Web Vitals and SEO


% 2021 Meenan vimeo 38:50






% -----------------------------------------------------------------------





\subsubsection{Custom Metrics}


% 2013 Meenan
- First, it is important to understand that no single number will answer that question. Even if you have defined exactly what you are trying to measure on your Web site, performance will vary widely across your user base and across the different pages on your site
- Nothing beats application-specific knowledge and measurements

% 2021 Meenan
- own / user specific metrics, e.g. time to first tweet for twitter 


% 2013 Grigork
- "there is no one single metric that holds true for every application, which means that we must carefully define custom metrics in each case"
-  "Custom and application-specific metrics are the key to establishing a sound performance strategy"







% -----------------------------------------------------------------------
% -----------------------------------------------------------------------




\subsection{The Metrics Taxonomy, Map}


% what about drawing something like a map of metrics? I can underlay the metrics on the map with Web Vitals, CRP, Navigation etc.


% What about those page weight metrics and number of requests etc ?







% ===== START: HERE I WILL LIST ALL METRICS =====



% https://developer.mozilla.org/en-US/docs/Glossary



DNS resolution / DNS lookup time % https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency
= GAs Avg. Domain Lookup Time (sec) % https://metriclabs.com.au/glossary/analytics-metrics/avg-domain-lookup-time-sec/


Connecting / TCP Handshake time % https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency


TLS Handshake time % https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency


Waiting / Server Response Time ?? % https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency


Receiving / Download Time ?? % https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency




onload Event: % https://developer.mozilla.org/en-US/docs/Web/API/GlobalEventHandlers/onload
% Defined by the HTML specification:



DOMContentLoaded Event % https://developer.mozilla.org/en-US/docs/Web/API/Window/DOMContentLoaded_event
% Defined by the HTML specification



Page Load Time: % https://hpbn.co/primer-on-web-performance/
- Has been the de facto metric of the web performance world
- Increasingly insufficient performance benchmark: we are no longer building pages, we are building dynamic and interactive web applications
% 2019 Enghardt



Start Render % https://designingforperformance.com/basics-of-page-speed/
- tells you how many seconds it took for the browser to begin rendering content


Time to First Byte: % https://developer.mozilla.org/en-US/docs/Glossary/time_to_first_byte

% 2014 Hogan
- First byte that the browser receives
- It’s a good indicator of how quickly the backend of your site is able to process and send back content


% https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
TTFB describes how long the requesting party has to wait from sending the first request packet until receiving the first response data packet







First Paint, Time to First Paint: % https://developer.mozilla.org/en-US/docs/Glossary/First_paint
% Defined by the Paint Timing API 


% 2013 Meenan
- can tell you the first point in time when the user sees something other than a blank white screen
- doesn’t necessarily mean the user sees anything useful




First Contentful Paint: % https://developer.mozilla.org/en-US/docs/Glossary/First_contentful_paint
% Defined by the Paint Timing API 



First Meaningful Paint: % https://developer.mozilla.org/en-US/docs/Glossary/first_meaningful_paint

% https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
The First Meaningful Paint is another user-centric metric and represents the point in time at which the largest visual change takes place; the underlying assumption here obviously is that the biggest visual change is relevant to the user, for example because a hero image or a navigation bar appear





Largest Contentful Paint: % https://wicg.github.io/largest-contentful-paint/

% https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
- approximation of the First Meaningful Paint that can be captured in the browser




Speed Index: % https://developer.mozilla.org/en-US/docs/Glossary/Speed_index

% https://medium.baqend.com/mobile-site-speed-measurement-best-practices-ff4a3f91b003
The Speed Index revolves around visual completeness and represents the average time it takes for website elements to become visible. It works well for websites with a static layout, but is unreliable for websites with moving elements like carousels or videos. Computing the Speed Index for this kind of website requires a custom timer or event to demarcate the point in time at which visual completion is reached

% 2013 Meenan
- best effort for representing the user experience in a single number



Above-the-fold time % 2013 Meenan
- point in time when the last visual change is made to the visible part of the page
- available only in lab environments where the visual progress of a page loading can be recorded





Time to Interactive: % https://developer.mozilla.org/en-US/docs/Glossary/Time_to_interactive






% ===== END: HERE I WILL LIST ALL METRICS =====






