\chapter{Approach}
\label{chapter:approach}

[tbd]

% [Last chapter]

% [This chapter: Describe shortly all sections from this chapter]

%- In this chapter the practical work should be documented and explained
%- Elaboration of how the practical work could help answer the research question
%- Discussion of real-life setup and how experiments approach it


% [In the next chapter]

%- Evaluation of data is in next chapter


% ------------------------------------------------------------------------------------------

\section{Introduction to Empirical Research Methods}

[tbd]

% Overview of methods

% reproduceability, validity etc

% Justification why following approaches are conducted as controlled experiments


% 2005 Sjoberg: experiments in computer science evaluation

% 2006 Wohlin
% Quantitative: Experiment, Case study, Survey, post-mortem analysis

% 2008_Mytkowicz Observer effect

% 2012 Maxion: Hallmarks of good experiment, Validity

% 2014 Tedre: Experimentation in computing


% 2016 Kohavi p. 4, p.5 planning an experiment


% --------------------------------------------------------


\subsection{Controlled Experiments in Computer Science}

[tbd]


% Short overview about controlled experiments in computer science
% Design: Show test setup image: Independent and dependent variables
% Hypothesis testing


% 2006 Wohlin:
% Design, analysis and interpretation

% 2014 Tedre: Controlled experiments

% Measuring Real User Performance in the Browser: Avoiding the Observer Effect



% ------------------------------------------------------------------------------------------
% ------------------------------------------------------------------------------------------


\section{Experiment Setup and Design}

% [Research Question and Design]

As stated in section X, a hypothesis of this work is that tracking tools such as RUM slow down websites.
In order to test the hypothesis, an experiment is implemented.
Figure X displays the different parts of the experiment:

\begin{enumerate}[label=(\alph*)]
\item The include of the tracking script is modelled by independent variables (IV).
\item How the IV affect performance is reflected in the dependent variables.
\item Through various combinations of the IVs, multiple test object variants (test websites) to test emerge. The test object itself, where the tracking script is added to, is an artificial generated e-commerce website.
\item Synthetic Monitoring is used to (1) collect data for the dependent variables, and (2) to generate traffic for the test websites.
\item RUM (the tracking script) is included into the test object by different variants. 
\end{enumerate}


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.8\textwidth]{design.pdf}
\caption{Components from the Controlled Experiment}
\label{figure:design_setup}
\end{center}
\end{figure}

The different parts of the experiment setup will be discussed in more detail below.


% Kohavi 2016: Sample size, collect right metrics, track right users, randomization unit


% --------------------------------------------------

\subsection{(a) Independent Variables}

% [What are they representing]

The IVs reflect two things:
How the tracking script is included into the website, and if another tracking script is present or not.
All IVs and the values they can take are listed in table X:

As described in section X, page tagging is the method of choice when including a tracking script into a website.
Page tagging is in essence the addition of a <script>-Tag to the main HTML document.
Different options exist when adding a script to the main document.
IV 1 describes the the position of the tracking script within the main document.
IV 2 describes the attribute of the included script tag.

If another tracking script, which may interfere with the "main" tracking script, is presented in the main document or not, is reflected in IV 3.

\begin{table}[h]
	\small
	\centering
	\begin{tabular}{ | l | l | l | }
	\hline
	IV \cellcolor{lightgrey} & Describes \cellcolor{lightgrey} & Possible Values \cellcolor{lightgrey} \\
	\hline
	1 & Position of the Tracking Script & top-head, bottom-head, bottom-body \\
	2 & Attribute of the Tracking Script & none, async, defer \\
	3 & Other Tracking Script included & no, yes \\
	\hline
	\end{tabular}
	\medskip
	\caption{Independent Variables}
	\label{table:independent_variables}
\end{table}


The three IVs are discussed in more detail in the following.
Other IVs considered, but not incorporated within the experiment, are discussed in section X.

% -------------------------

\subsubsection{IV 1: Position of the Tracking Script}

% [GA Tracking Code]

This IV may help understand if the position of the tracking script affects the performance.
Google Analytics (GA) is used as the tracking script.
In general, scripts can be included within the head or body section of the HTML document. % https://www.w3schools.com/js/js_whereto.asp
The GA documentation suggests that the GA script "should be added near the top of the <head> tag and before any other script or CSS tags".
% cite https://developers.google.com/analytics/devguides/collection/analyticsjs
Some details about the GA script are given in section X.

% [Other examples]

Other RUM vendors propose a similar position:
Hotjar suggests that "Paste the Tracking Code into the <head> section of your website."
And Akamai says for there tracking tool boomerang:
"Paste the following code snippet into every page of your site at the top of the HEAD, but after sensitive META tags."
% https://developer.akamai.com/tools/boomerang#dealing-with-script-error

The three possible positions of the tracking script, top-head, bottom-head, and bottom-body, are visualized in appendix X.


% https://speedcurve.com/setup/lux/
% SpeedCurve tracking script position: To add real user monitoring (RUM) to your site, paste this snippet at the top of the <HEAD> tag on your pages.

% Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
% And slides https://www.igvita.com/slides/2013/fluent-perfcourse.pdf


% -------------------------

\subsubsection{IV 2: Attribute of the Tracking Script}

The IV 2 is about the attribute of tracking script tag.
As explained in section X, possible attributes, next to have no attribute, are async and defer, and attributes are not applied on inline scripts.

As the GA script is included inline, different attributes should not affect the behaviour of the website nor its performance.
This IV should verify if this statement holds true and it is expected that changing this variable has no impact on performance.

The three attributes none, async, and defer are visualized in appendix X.


% -------------------------

\subsubsection{IV 3: Other Tracking Script is included}

Many websites use more than one tracking script to collect user data or even resort to custom tracking solutions.
Adding and loading another script within the website will increase the page weight, which leads to more requests and bytes, and depending for example on the network speed, loading an additional resource may impact performance badly.

The other script should not be any kind of script, but also a RUM solution, which has the objective to collect and report user data.
This IV is binary and models if another tracking script is included or not.
Changing this variable may capture if multiple tracking scripts interfere with each other.

Hotjar will be used as the additional tracking script.
The two variants, one with an additional script and one without, are visualized in appendix X.

% footnoe hotjar

% -------------------------

\subsubsection{Other IVs not considered but worth mentioning}

Many other IVs could be chosen and they could reflect variables of the tracking script itself, the website, or the infrastructure around it.

% [Script]

As for the additional IVs for the tracking script, other variables could reflect the quality or complexity of the tracking script, e.g., what kind of user data is collected and in what quantity.
Other variables may reflect the file size of the script, or compare different versions and implementations of the same RUM script.

% [Website]

Possible variables for the website may reflect implementation details, how the code is structured, which libraries are used, how many resources such as images and videos are being loaded, how they are arranged, how the CSS is handled, etc.

% [Infrastructure]

And in terms of infrastructure, IVs may model which servers are being used, which webservers run on them, the condition of the network, the device of the end users, or his operating system, browser version, and so on.
Especially on a website and infrastructure level, nearly countless IVs are imaginable.


% -------------------------

\subsubsection{Possible Variations of the Independent Variables}

% [Variants]

When putting the three defined IVs "Position", "Attribute", and "Other Script" together, different possible combinations emerge as displayed in table X.
The variants A1 and OS2 are redundant and will be dismissed as their IV configuration is already represented in variant P1.

\begin{table}[h]
	\small
	\centering
	\begin{tabular}{  | c || c | c | c | } 
	\hline
	Variant & Position & Attribute & Other Scripts \\
	\hline \hline
	Variant P1 & top-head & \cellcolor{lightgrey} none & \cellcolor{lightgrey} no \\
	   Variant P2 & bottom-head & \cellcolor{lightgrey} none & \cellcolor{lightgrey} no \\
	   Variant P3 & bottom-body & \cellcolor{lightgrey} none & \cellcolor{lightgrey} no \\
	  \hline
	   \sout{Variant A1} & \cellcolor{lightgrey} \sout{top-head} & \sout{none} & \cellcolor{lightgrey} \sout{no} \\
	   Variant A2 & \cellcolor{lightgrey} top-head & async & \cellcolor{lightgrey} no \\
	   Variant A3 & \cellcolor{lightgrey} top-head & defer & \cellcolor{lightgrey} no \\
	  \hline
	  \sout{Variant OS1} & \cellcolor{lightgrey} \sout{top-head} & \cellcolor{lightgrey} \sout{none} & \sout{no} \\
	  Variant OS2 & \cellcolor{lightgrey} top-head & \cellcolor{lightgrey} none & yes \\
	  \hline
	\end{tabular}
	\medskip
	\caption{Possible Test Object Variants when combining the IVs.}
	\label{table:test_object_variants}
\end{table}


% [What are the Variants]

The variants themselves are different index.html files from the test website.
The difference between the variants are only the options and position of the tracking script, as described above.
The rest of the test website is remaining the same over all variants.
How the test website is constructed, is discussed below.

% [Evaluation/Comparison]

For evaluation, I will compare the different possible values of one IV with each other, while the values of the other IVs remain unchanged.
For example, the different positions (IV 1) are compared with each other, while the attribute (IV 2) and whether or not another script is included (IV 3) remain the same for all positions.
This is needed in order to measure the impact when changing one IV.
Variants which are not from the same subgroup are not compared with each other, e.g. variant A2 will not be compared to variant OS2.


% --------------------------------------------------

\subsection{(b) Dependent Variables}

The dependent variables measure the outcome of the change in IVs with respect to web performance.
They are web performance metrics.
The synthetic monitoring and RUM solutions implemented in the experiment measure and deliver the dependent variables.

The dependent variables are visualized in order to facilitate the interpretation of the data.
Considered dependent variables (web performance metrics) are common web performance metrics as described in section X, such as page weight, navigation timing metrics such as TTFB and PLT, and user-centric metrics such as Speed Index or LCP.


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------

\subsection{(c) The Test Object: An E-Commerce Website}

% [Difficulty]

In the real world, each website is unique and follows its own design and architecture principles.
The difficulty for conducting research on websites such as within this thesis, is to find a common ground or general website that approximates real world examples as good as possible.

%TODO add best would be real life scenario ?

% Mimic the website is one thing, but also all other aspects should be fairly similar: infrastructure (hosting), hardware used, network speed, traffic, etc.
% This experiment/test is only an approximation and can serve as a starting point for further investigations

% Ideal: Use a real website. Is not possible


% [Goal]

One goal of this thesis is to create a test website which mimics a real life e-commerce website as good as possible.
Multiple ideas to achieve this goal are presented and discussed in the following.


% --------------------------------------------------------

\subsubsection{Minimal Website}

% [Idea]

The idea of the minimal website is to have a clean, laboratory-like environment which is in full control of the tester.
The minimal website is constructed without any content but only a link to an external script, which represents the tracking script.
The script itself has only place holder content which adds bytes to the file size, but does not execute any logic.
    
% [Setup]

The website is hosted, like all following test websites, on GitHub Pages, which is a quick, easy and fast way to deploy a website onto a real server.
The source code for the minimal website is available on GitHub, and the page is deployed to https://aramovski.github.io/.
% footnote : https://github.com/aramovski/aramovski.github.io
% Footnote https://pages.github.com/ [30.08.2021]

% [Evaluation]

The benefit of this approach is that the tester has a lot of control about the website, as there are not many complex coherences or variables within the website itself.
The three discussed IVs can easily be changed.
In addition, the script file size can be changed quickly.
As a downside, the minimal website is too far away from any real world website, e.g. no images or other resources are requested.
Also, it does not cover any e-commerce scenario.
Hence, this idea has been withdrawn.


% --------------------------------------------------------

\subsubsection{Median Website}

% [Idea]

The idea of the median website is to construct a website which represents an "average" website as good as possible in order to cover as many real life examples as possible.
The median website is created with a median page weight.
This means, it has an average amount of requests and bytes for each resource type such as images, fonts, or videos.
The data for the median values is taken from HTTPArchive. %footnote
The source code is available on GitHub and the page is deployed to https://aram-yesildeniz.github.io/median/.
%footnote source. https://github.com/aram-yesildeniz/median/tree/gh-pages

% [Evaluation]

The evaluation of the median website idea leads to the conclusion that this approach could only serve as a proof of concept or initial iteration for environment or other contextual components testing.
Additionally, it could serve as a basis for questions regarding page weight only.
The architecture of the median website is artifial, e.g. all resources are loaded in sequence.
There is no dynamic content, no navigation items or sub-pages, and no resemblance to an e-commerce website.
In short, the median website is too far away from reality.


% 2011 Grigorik Anatomy of modern web application
% - Chart with changes on time: apps are growing

% 2014 Hogan basics-of-page-speed/ What Is the HTTP Archive?


% --------------------------------------------------------

\subsubsection{WordPress Website}

% [Idea]

The idea of the WordPress website is to leverage one of the most widely used website building frameworks in order to cover many real world examples, which also rely on this content management system.
WordPress is roughly used by 40% of all websites.
% footnote https://w3techs.com/technologies/details/cm-wordpress

% [Plugins]

WordPress follows a plugin system which allows the operators of the website to easily integrate new features and tools into the website.
WooCommerce, an e-commerce plugin, is the most widely used plugin and used by nearly a fifth of all websites running on WordPress.
% footnote https://w3techs.com/technologies/details/cm-wordpress

% [Setup]

The WordPress website is constructed by downloading the latest version of WordPress and running it on the local webserver MAMP.
The idea of running the website on localhost is that it gives more control to the tester.
% footnote https://wordpress.org/download/
% footnote https://www.mamp.info/de/mac/
Additionally, WooCommerce and Google Analytics are integrated as plugins into the website.
Finally, WebPageTest is configured to measure and generate traffic for the website running on localhost.
%footnote https://www.monsterinsights.com/

% [Evaluation]

The idea of running WordPress on localhost was withdrawn:
Although the generated pages and templates from WordPress approximated real life websites, they could not compensate missing content and resources such as images or stylesheets.
As with the approaches described above, the WordPress website is too far away from a real life scenario.

%The rather unconventional setup to run WordPress on localhost and not on a common webhosting service.


% --------------------------------------------------------

\subsubsection{Mirroring a complete e-commerce website}

% [Idea]

The three approaches described above all have the same problem: they do not get sufficiently close to mimic real websites.
To counter attack this issue, the idea of mirroring a complete e-commerce website evolved, with the aim to approximate reality as close as possible.
Candidates for cloning are e-commerce websites with the biggest revenue in Germany.
% footnote https://de.statista.com/statistik/daten/studie/170530/umfrage/umsatz-der-groessten-online-shops-in-deutschland/
The first candidate is https://www.otto.de/, which is discarded as discussed below.
The second candidate https://www.zalando.de/ is used as the test object for the experiment.
Both, Otto and Zalando, are within the top three of e-commerce websites with highest revenue.

% [Setup]

Multiple tools exist to clone, mirror or download a complete website.
Tools used here are HTTrack and the Google Chrome extensions "Save All Resources"
As with the other approaches, the downloaded website will then be hosted on GitHub Pages.
% footnote https://www.httrack.com/
% footnote https://chrome.google.com/webstore/detail/save-all-resources/abpdnfjocnmdomablahdcfnoggeeiedb?hl=en


% --------------

\paragraph{Otto}

% [Manual Adjustments]

The naive approach is to download https://www.otto.de/ via HTTrack and move the content to GitHub Pages, which serves the website.
Some issues evolved with this approach, which were mostly cookie related issues and probably dynamic created resources, that could not be found and served.
This caused the web server to return more than a few 404s.

Additionally, problems with caching in combination with WebPageTests repeat view feature occurred:
The free webhosting service offered by GitHub does not allow to configure the webserver.
Especially caching in terms of HTTP cache-control headers can not be adjusted.
Therefore, the webserver decides which resources should be cached by the browser.
The configuration is such that many resources served will be cached by the browser.
This behaviour differs from the behaviour found on https://www.otto.de/, where caching is disabled for many resources, and resources will be downloaded again from the server also for repeat views.
The problem is not only that caching behaviour differs between the test object and the original website, but also that WebPageTests repeat view runs are failing when encountering too many missing resources.
Hence, data for repeat views is corrupt and can not be collected.

% Add Workaround ?
% Therefore i mock the missing ressources so that WPT can run bulk tests successfully.
% It is not possible to change the github request headers. We can modify the http request headers via html, but this is not a clean solution.
% Not possible to set cache control in html meta tag https://html.spec.whatwg.org/multipage/semantics.html#attr-meta-http-equiv


% [Evaluation]

Due to the caching problems and the failing repeat view runs in WebPageTest, this approach was declined.
Ideally, the test website is hosted on a similar infrastructure like the original website, with similar webserver configurations.
For economic and effort reasons this is not feasible, therefore another original website to clone with less caching issued has to be taken into account.

[more here ?]

% - user-set-consent-id-cookie: Cookie with name consentId is not set, user-set-consent-id-cookie returns therefore 404
% - subscribeToNewsletterSnippetContent: Change path did not work...
% - amount.json: Not found, also wl\_miniWishlistAmount in local storage does not created
% - a\_info: Mock a\_info response json does not work...

% - footer not visible
% - userTiming


% - mock image sprite\_all\_1ba408b2.png
% - create empty file called user-set-consent-id-cookie
% - change path for subscribeToNewsletterSnippetContent: This will remove the cookie banner... but then WPT works


% -------------------

%TODO add Compare original otto website with mock ?
% \subparagraph{Comparison to original webpage}
% Maybe use here just WPTs comparison tool
% Show some diagrams here
% The other diagrams with Zalando should go into evaluation chapter


% --------------------------------------------------------

\paragraph{Zalando}

% [Why]

% Idea: It looks like zalando page does not has that many cache-control headers, therefore it may be easier to clone so that RVs are more similar.

% Luckily, Zalano has different caching... Therefore I use a different e-commerce website which does not shut down caching so that the RV results are more similar.

% [Setup]

% use plugin to download page
% host it on github pages

% [Evaluation]


% see evaluation chapter
Similar enough...
Page weight figure X.
PLT figure X.
FCP figure X.


\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{original_screenshot.png}
		\caption{https://www.zalando.de/}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{test_screenshot.png}
		\caption{https://aram-yesildeniz.github.io/}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Original vs Test Object}
	\label{figure:plt_original_test}
\end{figure}




% -----------------

[Comparison Table ?]

%TODO Conclusion ?
% add some comparison table? like all ideas, with columns such as "control", "complexity", "generaliseabilty", "tool used / relies on", "tool support": httrack, plugin, httarchive, wordpress, ..." etc. ?


% --------------------------------------------------


\subsection{(d) Synthetic Monitoring: Create Traffic and Collect Data}

% Synthetic monitoring: Double role: creats traffic which is captured by RUM, and also measured the website

% I want to collect Lab and field data for dependent variables for comparison
% This setup is a special case because lab bots (e.g. WPT) simulate at the same time real users for RUM data


% -------------------------------------

\subsubsection{WebPageTest}

% [Setup]

% Possibilites:
% use public version
% API

% Private:
%- preconfigured Amazon Web Services (AWS) AMIs
%- localhost Docker


\paragraph{Configuration}

% already described in section X.

% [FV vs RV]

% First View: "First View refers to the cold cache setup in which nothing is served locally"
%Repeat View: "Repeat View refers to the warm cache containing everything instantiated by the first view" (2016 Using WPT p. 62)


\begin{table}[h]
	\small
	\centering
	\begin{tabular}{  | c | c | } 
	\hline
	\cellcolor{lightgrey} Configuration Setting & \cellcolor{lightgrey} Option \\
	\hline
%	Test Location & Test Location \\ 
	Browser & Chrome \\
	\hline
	Connection & LAN (connection speed is determined by hotspot) \\
	Desktop Browser Dimensions & default (1366x768) \\
	Number of Tests to Run & 1 \\
	Repeat View & First View and Repeat View \\
	Capture Video & True \\
	Keep Test Private & False \\
	Label & none \\
	\hline	  
	Advanced Tab & Nothing selected \\
	Chromium Tab & Capture Dev Tools Timeline selected  \\
	Auth, Script, Block, SPOF, Custom Tabs & Nothing  \\
	Bulk Testing Tab & URL of the test website $x$ times according to test plan \\
	\hline
	\end{tabular}
	\medskip
	\caption{WPT Configuration}
	\label{table:wpt_configuration}
\end{table}



% ----------------------


\paragraph{Traffic Shaping}

% [Why]

% Network Condition is very important for performance
% Already described in section X issue with Latency vs Bandwidth

% Overview table with different settings:
% https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency


% Important to have stable and realistic network condition
% Chromes tool is not the best for this % see blogpost https://calendar.perfplanet.com/2016/testing-with-realistic-networking-conditions/


% [Network Link Conditioner]

% Private WPT Instance docker on mac does not allow traffic shaping functionality from WPT
% One approach: I use Network Link Conditioner from Apple to slow down the whole machine. See in same blogpost that Patrick highly recommends this
% WPT also slows down their whole machines % https://forums.webpagetest.org/t/measure-internet-speed/11593
% In general internet connection is very unstable. If i run network link conditionier with e.g. DSL each speedtest gives different results. And other test platforms such as fast.com gives also different result.
% i will use the durchschnitt in germany which seems to be 40 mbit per second. or actually i use LTE profile from network conditioner which is 50 mbit per second 


% [Hotspot]

% Solution: Use Mobile Hotspot with 50 mbit/s
% as long as internet connection is stable along all tests, it should not make a big difference because i compare the different variants. Therefore internet connection will fall out of the equation


% ----------------------

\paragraph{Usage of the Bulk Test Feature}


% Bulk testing is a feature for private instances only
% actually gives pretty nice comparison graphics
% Misuse this feature to test the same website X times

% Measured metrics are exposed through a variety of files
% Ill download csv files Summary and detail ? TODO check again how this is called

% keep in mind that FV and RV for one test are directly next to each other
% so to get data for all FVs, i would need data from rows 1 3 5 etc

% i use this for quick comparison of original to test object


% --------------------------------------------------


\subsection{(e) Real-User Monitoring: Google Analytics}

% 2 functions: RUM itself is represented by the script, so it is basically the object where the IVs are applied to
% but also its used to collect data

% Setup: Use Unversial Analytics
% Set sample rate to 100% for speed metrics

% show code here

% Data: could get it from GA dashboard / reports
% but ill use the API to fetch the data more easily: https://aram-yesildeniz.github.io/google-analytics/

% what exactly is the script doing? https://developers.google.com/analytics/devguides/collection/analyticsjs/


What is the GA script doing?
Actually, it creates another async script tag which loads the actual analytics JS.
So basically it should not really play a role where we place it.
Evaluation will show.

What does the code snippet do?
Basically the sync part is only..
It creates another script tag which is async and which loads the tracking code.

This script is an inline script, as it does not have any src attribute.
In theory,  async and defer attributes on inline scripts are ignored, as explained in section X.
"Inline JavaScript script tags ignore the defer or async attribute"
As the "wrapper" snippet is a inline script (no src attribute), the attributes on this script tag should not have any effect.



% what exactly is the script doing? https://developers.google.com/analytics/devguides/collection/analyticsjs/


% where to include: https://developers.google.com/analytics/devguides/collection/analyticsjs/
% The Google Analytics tag should be added near the top of the <head> tag and before any other script or CSS tags



% ------------------------------------------------------------------------------------------
% ------------------------------------------------------------------------------------------


\section{Conducting the Experiment}


% [Schedule Runs]

% The Google Analytics code is more or less fixed and there are no configurations.
% It would be possible to change config of script, e.g. change sample rate, track other metrics etc.
% But it is not possible to change default tracking behaviour (?)

% How the script is included into the file should reflected withing Website Variations

% I will use only one WPT Configuration for all tests.
% Other WPT config can be used in future work, e.g. emulate mobile device.


\begin{table}[h]
	\small
	\centering
	\begin{tabular}{ | l | l | l | l | } 
	 \hline
	  Variant \cellcolor{lightgrey} &  Date \cellcolor{lightgrey} & Traffic Shaping \cellcolor{lightgrey} & Runs \cellcolor{lightgrey} \\
	  \hline
	  Original Website & 2021-05-28 & 50 Mbit/s & 500 \\
	  Mock without GA included & 2021-05-29 & " & " \\
	  \hline
	  Variant P1 & 2021-05-30 & " & " \\
	  Variant P2 & 2021-05-31 & " & " \\
	  Variant P3 & 2021-06-02 & " & " \\
	  \hline
	  Variant A2 & 2021-06-04 & " & " \\
	  Variant A3 & 2021-06-09 & " & " \\
	  \hline
	  Variant OS2 & 2021-06-03 & " & " \\
	  \hline
	  \end{tabular}
	\medskip
	\caption{Test Runs}
	\label{table:test_runs}
\end{table}


% [Procedure]

\begin{itemize}
\item Deploy variant of index.html by pushing to GitHub
% \item Start Network Link Conditioner with specified config on local machine
\item Test internet speed with speedtest-cli
\item Start local WPT server and agent
\item Configure WPT according to specified setup and add list of urls to bulk test interface
\item Run test
\item When finished, download summary csv file
\item On GA helper site, fetch and download data for the current day
\end{itemize}


% [Data Collection]


% [Data Analysis]

% those files are then used to analyse  and visualize data
% python code footnote github


% [Transition]

% Now lets see the data in chapter X Evaluation