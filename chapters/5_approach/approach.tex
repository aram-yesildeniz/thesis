\chapter{Approach}
\label{chapter:approach}

[tbd]

% [Last chapter]

% [This chapter: Describe shortly all sections from this chapter]

%- In this chapter the practical work should be documented and explained
%- Elaboration of how the practical work could help answer the research question
%- Discussion of real-life setup and how experiments approach it


% [In the next chapter]

%- Evaluation of data is in next chapter


% ------------------------------------------------------------------------------------------

\section{Introduction to Empirical Research Methods}

[tbd]

% Overview of methods

% reproduceability, validity etc

% Justification why following approaches are conducted as controlled experiments


% 2005 Sjoberg: experiments in computer science evaluation

% 2006 Wohlin
% Quantitative: Experiment, Case study, Survey, post-mortem analysis

% 2008_Mytkowicz Observer effect

% 2012 Maxion: Hallmarks of good experiment, Validity

% 2014 Tedre: Experimentation in computing


% 2016 Kohavi p. 4, p.5 planning an experiment


% --------------------------------------------------------


\subsection{Controlled Experiments in Computer Science}

[tbd]


% Short overview about controlled experiments in computer science
% Design: Show test setup image: Independent and dependent variables
% Hypothesis testing


% 2006 Wohlin:
% Design, analysis and interpretation

% 2014 Tedre: Controlled experiments

% Measuring Real User Performance in the Browser: Avoiding the Observer Effect



% ------------------------------------------------------------------------------------------
% ------------------------------------------------------------------------------------------


\section{Experiment Setup and Design}

% [Research Question and Design]

As stated in section X, a hypothesis of this work is that tracking tools such as RUM slow down websites.
In order to test the hypothesis, an experiment is implemented.
Figure X displays the different parts of the experiment:

\begin{enumerate}[label=(\alph*)]
\item The include of the tracking script is modelled by independent variables (IV).
\item How the IV affect performance is reflected in the dependent variables.
\item Through various combinations of the IVs, multiple test object variants (test websites) to test emerge. The test object itself, where the tracking script is added to, is an artificial generated e-commerce website.
\item Synthetic Monitoring is used to (1) collect data for the dependent variables, and (2) to generate traffic for the test websites.
\item RUM (the tracking script) is included into the test object by different variants. 
\end{enumerate}


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.8\textwidth]{design.pdf}
\caption{Components from the Controlled Experiment}
\label{figure:design_setup}
\end{center}
\end{figure}

The different parts of the experiment setup will be discussed in more detail below.


% Kohavi 2016: Sample size, collect right metrics, track right users, randomization unit


% --------------------------------------------------

\subsection{(a) Independent Variables}

% [What are they representing]

The IVs reflect two things:
How the tracking script is included into the website, and if another tracking script is present or not.
All IVs and the values they can take are listed in table X:

As described in section X, page tagging is the method of choice when including a tracking script into a website.
Page tagging is in essence the addition of a <script>-Tag to the main HTML document.
Different options exist when adding a script to the main document.
IV 1 describes the the position of the tracking script within the main document.
IV 2 describes the attribute of the included script tag.

If another tracking script, which may interfere with the "main" tracking script, is presented in the main document or not, is reflected in IV 3.

\begin{table}[h]
	\small
	\centering
	\begin{tabular}{ | l | l | l | }
	\hline
	IV \cellcolor{lightgrey} & Describes \cellcolor{lightgrey} & Possible Values \cellcolor{lightgrey} \\
	\hline
	1 & Position of the Tracking Script & top-head, bottom-head, bottom-body \\
	2 & Attribute of the Tracking Script & none, async, defer \\
	3 & Other Tracking Script included & no, yes \\
	\hline
	\end{tabular}
	\medskip
	\caption{Independent Variables}
	\label{table:independent_variables}
\end{table}


The three IVs are discussed in more detail in the following.
Other IVs considered, but not incorporated within the experiment, are discussed in section X.

% -------------------------

\subsubsection{IV 1: Position of the Tracking Script}

% [GA Tracking Code]

This IV may help understand if the position of the tracking script affects the performance.
Google Analytics (GA) is used as the tracking script.
In general, scripts can be included within the head or body section of the HTML document. % https://www.w3schools.com/js/js_whereto.asp
The GA documentation suggests that the GA script "should be added near the top of the <head> tag and before any other script or CSS tags".
% cite https://developers.google.com/analytics/devguides/collection/analyticsjs
Some details about the GA script are given in section X.

% [Other examples]

Other RUM vendors propose a similar position:
Hotjar suggests that "Paste the Tracking Code into the <head> section of your website."
And Akamai says for there tracking tool boomerang:
"Paste the following code snippet into every page of your site at the top of the HEAD, but after sensitive META tags."
% https://developer.akamai.com/tools/boomerang#dealing-with-script-error

The three possible positions of the tracking script, top-head, bottom-head, and bottom-body, are visualized in appendix X.


% https://speedcurve.com/setup/lux/
% SpeedCurve tracking script position: To add real user monitoring (RUM) to your site, paste this snippet at the top of the <HEAD> tag on your pages.

% Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
% And slides https://www.igvita.com/slides/2013/fluent-perfcourse.pdf


% -------------------------

\subsubsection{IV 2: Attribute of the Tracking Script}

The IV 2 is about the attribute of tracking script tag.
As explained in section X, possible attributes, next to have no attribute, are async and defer, and attributes are not applied on inline scripts.

As the GA script is included inline, different attributes should not affect the behaviour of the website nor its performance.
This IV should verify if this statement holds true and it is expected that changing this variable has no impact on performance.

The three attributes none, async, and defer are visualized in appendix X.


% -------------------------

\subsubsection{IV 3: Other Tracking Script is included}

Many websites use more than one tracking script to collect user data or even resort to custom tracking solutions.
Adding and loading another script within the website will increase the page weight, which leads to more requests and bytes, and depending for example on the network speed, loading an additional resource may impact performance badly.

The other script should not be any kind of script, but also a RUM solution, which has the objective to collect and report user data.
This IV is binary and models if another tracking script is included or not.
Changing this variable may capture if multiple tracking scripts interfere with each other.

Hotjar will be used as the additional tracking script.
The two variants, one with an additional script and one without, are visualized in appendix X.

% footnoe hotjar

% -------------------------

\subsubsection{Other IVs not considered but worth mentioning}

Many other IVs could be chosen and they could reflect variables of the tracking script itself, the website, or the infrastructure around it.

% [Script]

As for the additional IVs for the tracking script, the variables can reflect the quality or complexity of the tracking script, e.g., what kind of user data is collected and in what quantity.
Other variables may compare different versions and implementations of the same RUM script.

% [Website]

Possible variables for the website may reflect implementation details, how the code is structured, which libraries are used, how many resources such as images and videos are being loaded, how they are arranged, how the CSS is handled, etc.

% [Infrastructure]

And in terms of infrastructure, IVs may model which servers are being used, which webservers run on them, the condition of the network, the device of the end users, or his operating system, browser version, and so on.
Especially on a website and infrastructure level, nearly countless IVs are imaginable.


% -------------------------

\subsubsection{Possible Variations of the Independent Variables}

Combining the three defined IVs, different variants emerge as display in table X.

% as described before, i will compare the values within one independent variable. 
% This is needed in order to compare the impact of the different values within one IV.
% For example, i want to measure if there is a difference in performance between the different script attributes. To measure this, i set the default values for the other IVs and vary the values for the IV attribute


\begin{table}[h]
	\small
	\centering
	\begin{tabular}{  | c || c | c | c | } 
	\hline
	Variant & Position & Attribute & Other Scripts \\
	\hline \hline
	Variant P1 & top-head & \cellcolor{lightgrey} none & \cellcolor{lightgrey} no \\
	   Variant P2 & bottom-head & \cellcolor{lightgrey} none & \cellcolor{lightgrey} no \\
	   Variant P3 & bottom-body & \cellcolor{lightgrey} none & \cellcolor{lightgrey} no \\
	  \hline
	   \sout{Variant A1} & \cellcolor{lightgrey} \sout{top-head} & \sout{none} & \cellcolor{lightgrey} \sout{no} \\
	   Variant A2 & \cellcolor{lightgrey} top-head & async & \cellcolor{lightgrey} no \\
	   Variant A3 & \cellcolor{lightgrey} top-head & defer & \cellcolor{lightgrey} no \\
	  \hline
	  \sout{Variant OS1} & \cellcolor{lightgrey} \sout{top-head} & \cellcolor{lightgrey} \sout{none} & \sout{no} \\
	  Variant OS2 & \cellcolor{lightgrey} top-head & \cellcolor{lightgrey} none & yes \\
	  \hline
	\end{tabular}
	\medskip
	\caption{Test Object Variants}
	\label{table:test_object_variants}
\end{table}

% [Table Explanation]

Some variants are there twice:
Hence i will cut out variants X and Y.


% [Comparison]

In the evaluation, I will compare the different possible values of one IV, while the values of the other IVs remain unchanged.
For example, the different positions (IV 1) are compared with each other, while the attribute (IV 2) and whether or not another script is included (IV 3) remain the same for all positions.


% [Comparison]

% I will not compare variants which are not from the same subgroup, e.g. Variant A2 will not be compared to Variant OS2.
% Because the first row of the variants table also includes the default values for Attribute and Other Scripts, VP1 is equal to VA1 and VO1.

% With the defined IVs and variants, I can create the test objects, that is the index.html files with the corresponding setup.
% Because its easier to differentiate i will create for the three equal variants nevertheless own index files.

% For each test variant, I will create a concrete test artefact, which is a modified index.html.
% This index.html needs to be uploaded to the webserver before starting with the tests.

% All variants will have the same name which is index.html. This is the default file which will be delivered by the webserver when accessing root path of webpage.



% --------------------------------------------------


\subsection{(b) Dependent Variables}

% Measure effects
% Performance metrics from Lab and Field, see terms and definitions
% But also quality of RUM data. Because we could have a nice performance but RUM will be of bad quality

%TODO maybe i dont need own sections for those. Reference here where they are explained
\subsubsection{Page Weight}

	- Bytes
	- Requests

\subsubsection{Measures from Synthetic Monitoring}

Categories explained in section X.
WPT measures this in section X.

I will measure this:

- Navigation Timing Metrics:
	- TTFB
	- Document Complete
	- Fully Loaded
	
- User-Centric Metrics:
	- FCP
	- Speed Index
	- Core Web Vitals: LCP, CLS, TBT (proxy for FID)

\subsubsection{Measures from RUM}

GA tracks this, as explained in section X.

- PLT
- Page Download Time
- Server Connection Time
- Domain Lookup Time
- Redirection Time
- Server Response Time
- Domain interactive time
- Domain Content Loaed Time



Measured by WPT and GA:
- page load time
- domain lookup time
- page download time
- redirection time
- server connection time
- server response time
- Dom interactive time
- Dom content loaded time




% -------------------------------------------------------------------------
% -------------------------------------------------------------------------

\subsection{(c) The Test Object: An E-Commerce Website}

In the real world, each website is unique and follows its own design and architecture principles.
The difficulty for conducting research on websites such as within this thesis, is to find a common ground or general website that approximates real world examples as good as possible.

% is a goal of this thesis

% Ideal: Use a real website. Is not possible
% Mimic the website is one thing, but also all other aspects should be fairly similar: infrastructure (hosting), hardware used, network speed, traffic, etc.
% This experiment/test is only an approximation and can serve as a starting point for further investigations

% Concrete File to mimic: HTML Template / document with all other resources that build up the website: CSS, JS, Fonts, Images, Videos, etc.

% Depending on different approaches / Ideas (see next chapter), template looks different
% But general structure stays the same and independent variables can be defined
% Here we show different independent variables and variants

% Several ideas are proposed
% Each idea has pro and contra: each idea should be discussed of its usefulness, advantages and disadvantages

% skeletal explained in section X, wordpress in section X, etc.


% --------------------------------------------------------

\subsubsection{Plain / Skeletal Website}


% [Why]
% Idea: Lab environment to have control over all and see effects of changing independent variables
% Use this as the simplest test possible, not even POC (POC is http archive site)
% boil it down to the script only: use different script sizes and see what happens when including them
    
% [Setup]
% website: simple html document
% source code: https://github.com/aramovski/aramovski.github.io
% Hosting: github pages: https://aramovski.github.io/

% [Evaluation]
% Information gained from this experiment
% Limitations and questions which can not be answered with this approach
% Problem: Too far away from reality

% --------------------------------------------------------

\subsubsection{"Median" Website from HTTP Archive}


% [Why]
% Idea: Get correct page weight

% [Setup]
% website: mimic page weight as reported on http archive
% source. https://github.com/aram-yesildeniz/median/tree/gh-pages
% Hosting: github pages: https://aram-yesildeniz.github.io/median/

% [Evaluation]
% Only good as a POC: Show that changing independent variables X affect result
% too far away from reality
% maybe compare with real website ?


% 2011 Butkiewicz: Correlation between website complexity and its performance:
% -> Therefore it makes sense for me to use http archive inspired website approach, since page weight / page complexity impacts performance
% - Measures impact of website complexity on performance
% - Finds out that top 5 metrics that determine performance are:
% total number of objects loaded,  the number of these objects that are Javascripts,  the total web- page size,  and the number of servers and origins contacted in loading objects on the page.


% 2011 Grigorik Anatomy of modern web application
% - Chart with changes on time: apps are growing


% 2014 Hogan basics-of-page-speed/ What Is the HTTP Archive?


% --------------------------------------------------------

\subsubsection{WordPress}


% [Why]
% Show usage of WordPress with some statistics: Why is it so verbreitet

% [Plugins]
% Explain Plugin system

% WooCommerce

% [Setup]
% Explain Setup on localhost with wocommerce and GA plugin

% [Evaluation]
% Elaborate why this idea was not used



% --------------------------------------------------------


\subsubsection{Mirroring a complete e-commerce website}


% [Why]

% Close to reality as possible


% Idea: Close to reality as possible
% Problems when mirroring a website
% Elaborate why this idea of mirroring complete website was not used
% I used mock of start page of otto, which works fine
% Compare original otto website with mock


% Why Otto and Zalando ?Which website / shop to clone? Show some statistics about biggest e-commerce websites in germany




% [Setup]

% Tool support to mirror / download complete website: httrack and chrome plugin
% Use free hosting service to serve website

% Otto first try
% Zalando final version

% [Evaluation]
% see evaluation chapter


% --------------

\paragraph{Otto}


% [Manual Adjustments]

% Manual adjustments needed:
% - Move everything to test folder because top domain is /otto


% What did not work (mostly 404s):
% - user-set-consent-id-cookie: Cookie with name consentId is not set, user-set-consent-id-cookie returns therefore 404
% - subscribeToNewsletterSnippetContent: Change path did not work...
% - amount.json: Not found, also wl\_miniWishlistAmount in local storage does not created
% - a\_info: Mock a\_info response json does not work...

% - footer
% - userTiming


% WPT RV is returning empty csv when 404s are encountered.
% Therefore i mock the missing ressources so that WPT can run bulk tests successfully.

% - mock image sprite\_all\_1ba408b2.png

% - create empty file called user-set-consent-id-cookie

% - change path for subscribeToNewsletterSnippetContent: This will remove the cookie banner... but then WPT works



% -------------------

\subparagraph{Comparison to original webpage}

% Maybe use here just WPTs comparison tool

% Show some diagrams here
% The other diagrams with Zalando should go into evaluation chapter



% -----------

\subparagraph{Problem with Caching and Repeat View}

% - Problem with RV, Caching:
% Otto sets request headers to cache-control: no-cache which means that RV basically downloads all resources again.
% The mock is hosted on Github, where the cache-control header is set to ...
% It is not possible to change the github request headers. We can modify the http request headers via html, but this is not a clean solution.
% Therefore I use a different e-commerce website which does not shut down caching so that the RV results are more similar.

% Ideally I would host the mock website on a similar infrastructure as the original site with the same webserver configuration. This is for a masters thesis not feasible.

% Not possible to set cache control in html meta tag https://html.spec.whatwg.org/multipage/semantics.html#attr-meta-http-equiv


% --------------------------------------------------------

\paragraph{Zalando}

% [Why]

% Idea: It looks like zalando page does not has that many cache-control headers, therefore it may be easier to clone so that RVs are more similar.

% [Setup]

% use plugin to download page
% host it on github pages

% [Evaluation]


% add screenshots of original vs test
% see evaluation chapter


\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{original_screenshot.png}
		\caption{https://www.zalando.de/}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{test_screenshot.png}
		\caption{https://aram-yesildeniz.github.io/}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Original vs Test Object}
	\label{figure:plt_original_test}
\end{figure}




% --------------------------------------------------


\subsection{(d) Synthetic Monitoring: Create Traffic and Collect Data}

% Synthetic monitoring: Double role: creats traffic which is captured by RUM, and also measured the website

% I want to collect Lab and field data for dependent variables for comparison
% This setup is a special case because lab bots (e.g. WPT) simulate at the same time real users for RUM data


% -------------------------------------

\subsubsection{WebPageTest}

% [Setup]

% Possibilites:
% use public version
% API

% Private:
%- preconfigured Amazon Web Services (AWS) AMIs
%- localhost Docker


\paragraph{Configuration}

% already described in section X.

% [FV vs RV]

% First View: "First View refers to the cold cache setup in which nothing is served locally"
%Repeat View: "Repeat View refers to the warm cache containing everything instantiated by the first view" (2016 Using WPT p. 62)


\begin{table}[h]
	\small
	\centering
	\begin{tabular}{  | c | c | } 
	\hline
	\cellcolor{lightgrey} Configuration Setting & \cellcolor{lightgrey} Option \\
	\hline
%	Test Location & Test Location \\ 
	Browser & Chrome \\
	\hline
	Connection & LAN (connection speed is determined by hotspot) \\
	Desktop Browser Dimensions & default (1366x768) \\
	Number of Tests to Run & 1 \\
	Repeat View & First View and Repeat View \\
	Capture Video & True \\
	Keep Test Private & False \\
	Label & none \\
	\hline	  
	Advanced Tab & Nothing selected \\
	Chromium Tab & Capture Dev Tools Timeline selected  \\
	Auth, Script, Block, SPOF, Custom Tabs & Nothing  \\
	Bulk Testing Tab & URL of the test website $x$ times according to test plan \\
	\hline
	\end{tabular}
	\medskip
	\caption{WPT Configuration}
	\label{table:wpt_configuration}
\end{table}



% ----------------------


\paragraph{Traffic Shaping}

% [Why]

% Network Condition is very important for performance
% Already described in section X issue with Latency vs Bandwidth

% Overview table with different settings:
% https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency


% Important to have stable and realistic network condition
% Chromes tool is not the best for this % see blogpost https://calendar.perfplanet.com/2016/testing-with-realistic-networking-conditions/


% [Network Link Conditioner]

% Private WPT Instance docker on mac does not allow traffic shaping functionality from WPT
% One approach: I use Network Link Conditioner from Apple to slow down the whole machine. See in same blogpost that Patrick highly recommends this
% WPT also slows down their whole machines % https://forums.webpagetest.org/t/measure-internet-speed/11593
% In general internet connection is very unstable. If i run network link conditionier with e.g. DSL each speedtest gives different results. And other test platforms such as fast.com gives also different result.
% i will use the durchschnitt in germany which seems to be 40 mbit per second. or actually i use LTE profile from network conditioner which is 50 mbit per second 


% [Hotspot]

% Solution: Use Mobile Hotspot with 50 mbit/s
% as long as internet connection is stable along all tests, it should not make a big difference because i compare the different variants. Therefore internet connection will fall out of the equation


% ----------------------

\paragraph{Usage of the Bulk Test Feature}


% Bulk testing is a feature for private instances only
% actually gives pretty nice comparison graphics
% Misuse this feature to test the same website X times

% Measured metrics are exposed through a variety of files
% Ill download csv files Summary and detail ? TODO check again how this is called

% keep in mind that FV and RV for one test are directly next to each other
% so to get data for all FVs, i would need data from rows 1 3 5 etc

% i use this for quick comparison of original to test object


% --------------------------------------------------


\subsection{(e) Real-User Monitoring: Google Analytics}

% 2 functions: RUM itself is represented by the script, so it is basically the object where the IVs are applied to
% but also its used to collect data

% Setup: Use Unversial Analytics
% Set sample rate to 100% for speed metrics

% show code here

% Data: could get it from GA dashboard / reports
% but ill use the API to fetch the data more easily: https://aram-yesildeniz.github.io/google-analytics/

% what exactly is the script doing? https://developers.google.com/analytics/devguides/collection/analyticsjs/


What is the GA script doing?
Actually, it creates another async script tag which loads the actual analytics JS.
So basically it should not really play a role where we place it.
Evaluation will show.

What does the code snippet do?
Basically the sync part is only..
It creates another script tag which is async and which loads the tracking code.

This script is an inline script, as it does not have any src attribute.
In theory,  async and defer attributes on inline scripts are ignored, as explained in section X.
"Inline JavaScript script tags ignore the defer or async attribute"
As the "wrapper" snippet is a inline script (no src attribute), the attributes on this script tag should not have any effect.



% what exactly is the script doing? https://developers.google.com/analytics/devguides/collection/analyticsjs/


% where to include: https://developers.google.com/analytics/devguides/collection/analyticsjs/
% The Google Analytics tag should be added near the top of the <head> tag and before any other script or CSS tags



% ------------------------------------------------------------------------------------------
% ------------------------------------------------------------------------------------------


\section{Conducting the Experiment}


% [Schedule Runs]

% The Google Analytics code is more or less fixed and there are no configurations.
% It would be possible to change config of script, e.g. change sample rate, track other metrics etc.
% But it is not possible to change default tracking behaviour (?)

% How the script is included into the file should reflected withing Website Variations

% I will use only one WPT Configuration for all tests.
% Other WPT config can be used in future work, e.g. emulate mobile device.


\begin{table}[h]
	\small
	\centering
	\begin{tabular}{ | l | l | l | l | } 
	 \hline
	  Variant \cellcolor{lightgrey} &  Date \cellcolor{lightgrey} & Traffic Shaping \cellcolor{lightgrey} & Runs \cellcolor{lightgrey} \\
	  \hline
	  Original Website & 2021-05-28 & 50 Mbit/s & 500 \\
	  Mock without GA included & 2021-05-29 & " & " \\
	  \hline
	  Variant P1 & 2021-05-30 & " & " \\
	  Variant P2 & 2021-05-31 & " & " \\
	  Variant P3 & 2021-06-02 & " & " \\
	  \hline
	  Variant A2 & 2021-06-04 & " & " \\
	  Variant A3 & 2021-06-09 & " & " \\
	  \hline
	  Variant OS2 & 2021-06-03 & " & " \\
	  \hline
	  \end{tabular}
	\medskip
	\caption{Test Runs}
	\label{table:test_runs}
\end{table}


% [Procedure]

\begin{itemize}
\item Deploy variant of index.html by pushing to GitHub
% \item Start Network Link Conditioner with specified config on local machine
\item Test internet speed with speedtest-cli
\item Start local WPT server and agent
\item Configure WPT according to specified setup and add list of urls to bulk test interface
\item Run test
\item When finished, download summary csv file
\item On GA helper site, fetch and download data for the current day
\end{itemize}


% [Data Collection]


% [Data Analysis]

% python code footnote github


% [Transition]

% Now lets see the data in chapter X Evaluation