% TODO rename: Tools and Research ?
\chapter{Related Work}

\begin{itemize}
	\item Last chapter...
	\item This chapter: Describe shortly all sections from this chapter
	\item In the next chapter...
\end{itemize}

\begin{itemize}
\item This chapter should list research which covers and explores questions relevant for this thesis, such as:
	\begin{itemize}
	\item Metrics: New metrics, meaning of metrics, difficulties of defining metrics, etc.
	\item Overview, evaluation and comparison of measurement tools and methods
	\item If available: Impact of RUM on performance
	\end{itemize}
\end{itemize}


% ---------------------------------------------
% ---------------------------------------------


% 2016 Viscomi
% https://docs.webpagetest.org/

\section{WebPageTest}

\begin{itemize}
\item Overview
\item Configuration
\item Private Instances
\end{itemize}



% 2011 Grigorik https://hpbn.co/primer-on-web-performance/


\subsection{Overview}

\begin{itemize}
\item What is it
\item Why to use it, Who uses it, how to use it
\item Waterfall and Grades
\item See in performance tab for details about grades and optimization techniques
\end{itemize}


\subsection{Configuration}

% Meenans video as resource

\begin{itemize}
\item Caching, repeat view
\item Traffic shaping
\item e.g. capture devtools timeline
\end{itemize}



\subsection{Private Instances}

% Meenans video as resource

\begin{itemize}
\item Architecture
\item AWS
\item Docker localhost
\item Bulk tests
\end{itemize}



% Metrics? see terms and definitions ?


%TODO put this on Waterfall somewhere else? Maybe in Terms and Definitions ?
% [Waterfall: Resource Waterfall and Connection View]

% 2011 Grigorik Analyzing the Resource Waterfall in https://hpbn.co/primer-on-web-performance/


% 2014 Hogan ch 2
- A waterfall chart such as Figure 2-2 shows you how much time it takes to request the contents of a page, such as CSS, images, or HTML, and how much time it takes to download this content before displaying it in a browser.
- ...

% Differnce between conenction view and request view as described in section X.








% Better not use own chapter for WPT Metrics: Once i have written down all metrics and categorised them, i can describe here which tools expose what. WPT would be the biggest example for a synthetic monitoring tool

%WPT specific metrics which are not in the metrics chapter

\subsection{Metrics}

% https://docs.webpagetest.org/getting-started/

\begin{itemize}
\item Metrics Categories:
	\begin{itemize}
	\item High Level Metrics:
		\begin{itemize}
		\item Document Complete
		\item Fully Loaded
		\item Load Time
		\item First Byte
		\item Start Render
		\item Requests
		\item Bytes In (Page Size)
		\end{itemize}
	\item Page-level Metrics:
		\begin{itemize}
		\item Technical Page Metrics:
			\begin{itemize}
			\item -> APIs, GA Site Speed Metrics
			\item TTFB
			\item loadTime
			\item docTime
			\item ...
			\end{itemize}
		\item Visual Metrics:
			\begin{itemize}
			\item SpeedIndex
			\item firstPaint
			\item firstContentfulPaint
			\item firstMeaningfulPaint
			\item ...
			\end{itemize}
		\item Javascript and CPU timings
		\item Page Information
		\item Browser State
		\item Lighthouse Summary Metrics
		\item Optimization Checks/Grades
		\item Instrumented Metrics
		\item Test Information
		\item Misc
		\end{itemize}
	\item Request-level metrics:
		\begin{itemize}
		\item Request Details
		\item Request Timings
		\item Request Stats
		\item Headers
		\item Protocol Information
		\item Javascript/CPU details
		\item Optimization Checks
		\item Misc	
		\end{itemize}
	\end{itemize}
	
\item Optimization Grades:
	\begin{itemize}
	\item Keep-alive Enabled
	\item Compress Text
	\item Compress Images
	\item Cache Static Content
	\item Use of CDN
	\end{itemize}
\item First View and Repeat View
\end{itemize}

% Taken from WPT book p. 34, ...
% https://www.webpagetest.org/forums/showthread.php?tid=10315
% https://www.webpagetest.org/forums/showthread.php?tid=13266
% https://www.webpagetest.org/forums/showthread.php?tid=332
% https://www.webpagetest.org/forums/showthread.php?tid=10732
% https://www.webpagetest.org/forums/showthread.php?tid=12846



% Move those definitions to chapter above?
% Here i could only describe which metrics are available in WPT

\begin{center}
	\small
	\begin{longtable}{ p{0.4\linewidth} | p{0.6\linewidth} }
	Name & Description \\ 
	\hline
	Successful Tests & Amount of tests who completed successfully  \\
	
	Document Complete & The time from the initial request until the browser fires load event. Also known as the document complete time. This is the time at which the Document Object Model (DOM) has been created and all images have been downloaded and displayed. For most traditional web pages, the load time is a suitable metric for representing how long a user must wait until the page becomes usable. This is the default performance metric on WebPageTest. Also known as Load Time (?). Around this time, the page's script is hard at work in the load-event handler firing off more requests for secondary content. The incomplete nature of this metric is why Fully Loaded was added to the table of metrics from the previous section. window.onload (?). The point where the browser onLoad event fires. The equivalent Navigation Timing event is loadEventStart. Document Complete Time: Amount of time that has elapsed from the initial page request until the browser fires the load event. This is the time at which the Document Object Model (DOM) has been created and all images have been downloaded and displayed. \\
	
	Fully Loaded & The time from the initial request until WebPageTest determines that the page has finished loading content. The page might have waited for the load event to defer loading secondary content. The time it takes to load the secondary content is accounted for in the Fully Loaded Time. The time (in ms) the page took to be fully loaded — e.g., 2 seconds of no network activity after Document Complete. This will usually include any activity that is triggered by javascript after the main page loads. The point after onLoad where network activity has stopped for 2 seconds. Specific to WebPageTest and not provided by Performance API. Fully loaded waits for 2 seconds of no network activity (and no outstanding requests) after onLoad and then calls it done (only measures to the last activity, doesn't include the 2 seconds of silence in the measurement). Fully Loaded is a measure based on the network activity and is the point after onload when there was no activity for 2 seconds. \\
	
	First Byte & Time until the server responds with the first byte of the response.  \\
	
	Start Render & Time until the browser paints content to the screen. The time for the browser to display the first pixel of content (paint) on the screen. Time until the browser paints content to the screen. WebPageTest's own metric, determined by programmatically watching for visual changes to the page. Same as First Render? \\
	
	Bytes In (Doc) & Total size of the Document Complete Requests' response bodies in bytes.  \\
	
	Requests (Doc) & Number of HTTP requests before the load event, not including the initial request. \\
	
	Load Event Start & Time in ms since navigation started until window.onload event was triggered (from W3C Navigation Timing). \\
	
	Speed Index	& See Speed Index  \\
	
	Last Visual Change & Time in ms until the last visual changed occured. Last change is a completely visual measurement and is the last point in the test when something visually changed on the screen. It could be something as simple as an animated gif or ad even that didn't really cause much CPU work but changed some pixels on the screen. It is only captured when video is recorded because it depends on the video capture to measure it. \\
	
	Visually Complete & Time in ms when page was visually completed. Is measured from a video capture of the viewport loading and is the point when the visible part of the page first reached 100\% "completeness" compared to the end state of the test. \\
	\caption{Your caption here} % needs to go inside longtable environment
	\label{tab:myfirstlongtable}
	\end{longtable}
\end{center}



































% ---------------------------------------------
% ---------------------------------------------







\section{Google Analytics}

\begin{itemize}
    \item Custom metrics with Google Web Vitals as example
    \item Show how to include GA script (analytics.js, gtag, Tag Manager, etc.)
    \item Show some real life examples how script code is included into page, e.g. from Amazon, Otto etc
\end{itemize}

% Kessler 2012 p. 581
% marek 2011: glossary

% Grigorik Conference Talk https://www.youtube.com/watch?v=PkOBnYxqj3k&ab_channel=IlyaGrigorik
% And slides https://www.igvita.com/slides/2013/fluent-perfcourse.pdf
Real User Measurement (RUM) with Google Analytics


\subsection{The Tracking Script}

\begin{itemize}
\item Show multiple code examples
\item Explain whats going on: script tag, create script element etc.
\item Maybe also show Hotjar example to see that they are similar
\end{itemize}




% https://developers.google.com/analytics/devguides/collection/analyticsjs
- this async pattern is used so that all browsers will load it async
- we can just use async attribute for newer browsers...




% Compare how GA script is included into web site with some other analytics services examples
% Some other real life examples of RUM here? To demonstrate async... boomeraing readme for example has nice explanation







\subsection{Site Speed Metrics}


Show with analytics.js that it is indeed those navigation timing api calculations.

Ec = function (a)...

% 2013 Girgorik: "Google Analytics automatically gathers Navigation Timing data when the analytics tracker is installed." ch. primer on web performance



GA does not really provide any UX metrics! The site speed metrics are all from navigation timing api which are measurements from the browser.

GA Site Speed Metrics (description from \url{https://support.google.com/analytics/answer/2383341?hl=en&ref_topic=1282106})

\url{https://stackoverflow.com/questions/18972615/how-do-the-metrics-of-google-analytics-site-speed-map-to-the-w3c-navigation-timi}

\begin{center}
\small

	\begin{tabular}{ p{0.4\linewidth} | p{0.6\linewidth} }
	Name & Description  \\ 
	\hline
	Page Load Sample & The number of pageviews that were sampled to calculate the average page-load time.  \\
	
	Speed Metrics Sample & The sample set (or count) of pageviews used to calculate the averages of site speed metrics. This metric is used in all site speed average calculations, including avgDomainLookupTime, avgPageDownloadTime, avgRedirectionTime, avgServerConnectionTime, and avgServerResponseTime.  \\
	
	DOM Latency Metrics Sample & Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. This metric is used to calculate ga:avgDomContentLoadedTime and ga:avgDomInteractiveTime.  \\

	Page Load Time (sec) & The average amount of time (in seconds) it takes that page to load, from initiation of the pageview (e.g., click on a page link) to load completion in the browser.  \\
	
	Domain Lookup Time (sec) & The average amount of time spent in DNS lookup for the page.  \\
	
	Page Download Time (sec) & The time to download your page.  \\
	
	Redirection Time (sec) & The time spent in redirection before fetching the page. If there are no redirects, the value for this metric is expected to be 0.  \\
	
	Server Connection Time (sec) & The time needed for the user to connect to your server.  \\

	Server Response Time (sec) & The time for your server to respond to a user request, including the network time from the user's location to your server.  \\
	
	Document Interactive Time (sec) & The average time (in seconds) that the browser takes to parse the document (DOMInteractive), including the network time from the user's location to your server. At this time, the user can interact with the Document Object Model even though it is not fully loaded.  \\

	Document Content Loaded Time (sec) & The average time (in seconds) that the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded), including the network time from the user's location to your server. Parsing of the document is finished, the Document Object Model is ready, but referenced style sheets, images, and subframes may not be finished loading. This event is often the starting point for javascript framework execution, e.g., JQuery's onready() callback, etc.  \\
	\end{tabular}
\end{center}


%----------------------------------------------------------------



\subsection{Comparison GA and WPT Metrics}


\begin{sidewaysfigure}

\begin{center}
	\begin{tabular}{ l | l | l }
	Navigation Timing API & WPT & GA \\ 
	\hline
	loadEventStart - navigationStart & Document Complete, Load Event Start & pageLoadTime \\
	domainLookupEnd - domainLookupStart & DNS lookup, dns\textunderscore ms & domainLookupTime \\
	connectEnd - connectStart & connect\textunderscore ms & serverConnectionTime \\
	responseStart - requestStart & .. & serverResponseTime \\
	responseEnd - responseStart & .. & pageDownloadTime \\
	fetchStart - navigationStart & .. & redirectionTime \\
	domInteractive - navigationStart & .. & domInteractiveTime \\
	domContentLoadedEventStart - navigationStart & domContentLoadedEventStart & domContentLoadedTime \\
	\end{tabular}
\end{center}


\end{sidewaysfigure}




\begin{itemize}
\item We can show that above relations are true with experiments
\item Load test page on a specific day only once and save timings exposed by performance.timing object (from console)
\item Calculate differences corresponding to the table
\item Get GA data for that day and save it
\end{itemize}





















% ---------------------------------------------
% ---------------------------------------------



\section{Research}


\begin{itemize}
\item Research exists about topics like: ....
\item Here i will provide a list of in my eyes relevant papers, summaries them and discuss why this is important for my research
\end{itemize}

% TODO: Think about categorising papers


\subsection{some title for first category}

% 2014 Singal: WEB ANALYTICS: STATE-OF-ART & LITERATURE ASSESSMENT
\paragraph{2014 Singal}

I.
- Describes history of web analytics and tools
- Provides definitions and taxonomy for metrics
- Describes log file vs page tagging
- Describes KPIs

II.
- Lit. overview for KPIs and Web Metrics
- Lit. overview for "Trust"
- Lit. overview for "Fuzzy"
-> What are does categories?


III.
- Some other literature worth mentioning


IV.
- Describes 8 open challenges for researchers


% ---------------------------------------------


\paragraph{2015 Bekavac}

- Two parts:
	- 1: Some general overview of web analytics, tools and metrics, KPIs etc
	- 2: Empirical study about employees satisfaction of used web analytics tools

1: 
- 9 web business models and 5 common goals
- Hypothesis: Web analytics tools track and improve a user’s satisfaction with web-based business models.
- Web anayltics defintion. Log files vs Site Tagging
- Web Analytics process
- Tools: 5 categories, Process of selecting tool, Table with features of different tools
- Web metrics categories, Table with business models and their KPIs

2:
- Which tools are used for which purpose / Activity
- Users satisfaction



% ---------------------------------------------


\subsection{Research about Tools}



% https://www.kaushik.net/avinash/web-analytics-tool-selection-three-questions-to-ask-yourself/
\paragraph{Kaushik 2007}
- Provides 3 questions which help to choose web analytics tools


% ---------------------------------------------


% 2011 Nakatani: A web analytics tool selection method: an analytical hierarchy process approach
\paragraph{2011 Nakatani}

- Gives some arguments why web analytics is important for business
- Provides different categorizations for web analytics tools
- Gives pros and cons of log file analysis and page tagging
- Provides tool selection method based on AHP (Analytic Hierarchy Process)

"Web analytics tools collect click-stream data, track users navigation paths, process and present the data as meaningful information.
- Categorizations:
1: By 4 different data collection methods
2: SaaS vs in-house
3: mobile vs non-mobile
4: Time lag




% ---------------------------------------------


% 2013 Wang: WProf: Profiler / new tool for finding bottlenecks
% - PLT metric
% - How page loading works, CRP, dependencies
% - They create WProf as a tool to analyse dependencies and performance bottlenecks




% ---------------------------------------------


% 2016 Kaur: Tool comparison
\paragraph{2016 Kaur}
% This paper evaluated the university websites of Punjab using four automated tools and gives the comparative results of various factors using these tools.
% -





% 2009 Jansen ch. 6.3
% - paraphrases 10 tips by Kaushik... i would neet to cite them directly


% Croll 2009 Choosing a analytics platform p. 144
- free vs paid
- real time vs long term
- hosted vs in-house
- data portability


% 2011 Marek: Choose a program


% 2015 Bekavac --> Move this to related work ??
% - 5 categories of tools
% - Process of selecting tool
% - Table with features of tools


% 2016 Bartuskova: 1.2 Services for Website Speed Testing



% 2019 Kumar
- free / open source tools
- proprietary tools
- Service Hosted Software
- GA most popular one


% 2020 Quintel: Matomo --> put this to related work ??
% Lighthouse Performance Scoring ?




% Tools: Kessler 2016 p. 576 ??




% ---------------------------------------------


\subsection{Research about Metrics}


% 2018 Netravali: Ready Index: new metric




% 2019 Enghard Pitfalls







- Dont know:
% 2015  Cito: IDENTIFYING WEB PERFORMANCE DEGRADATIONS THROUGH SYNTHETIC AND REAL-USER MONITORING

% 2016 Bartuskova: Our results indicate that the choice of service and location affects significantly results of website speed testing.






















